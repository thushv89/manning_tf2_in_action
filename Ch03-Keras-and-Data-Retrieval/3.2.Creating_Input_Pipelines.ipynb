{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Keras and Data Retrieval in TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch03-Keras-and-Data-Retrieval/3.2.Creating_Input_Pipelines.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "fix_random_seed(4321)\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "\n",
    "data_dir = 'data'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `tf.data` API to retrieve data\n",
    "\n",
    "Here we will be using the `tf.data` API to feed a dataset containing images of flowers. The dataset has a folder containing the images and a CSV file listing filenames and their corresponding label as an integer. We will write a TensorFlow data pipeline that does the following.\n",
    "\n",
    "* Extract filenames and classes from the CSV\n",
    "* Read in the images from the extracted filenames and resize them to 64x64\n",
    "* Convert the class labels to one-hot encoded vectors\n",
    "* Combine the processed images and one-hot encoded vectors to a single dataset\n",
    "* Finally, shuffle the data and output as batches\n",
    "\n",
    "### Downloading the data\n",
    "The dataset is available at https://www.kaggle.com/olgabelitskaya/flower-color-images/data . \n",
    "\n",
    "You need to download the zip file available in this URL and place it in the `data` folder in the `Ch03-Keras-and-Data-Retrieval` folder. You **do not** need to extract it as the following code will do it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3.2\n",
    "\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Extracting the flowers image data to a directory\n",
    "zip_filepath = [os.path.join('data',f) for f in os.listdir('data') if f.endswith('.zip')]\n",
    "\n",
    "if len(zip_filepath)==0:\n",
    "    print(\"Did you download the dataset as a zip file and place it in the Ch03-Keras-and-Data-Retrieval/data folder?\")\n",
    "elif len(zip_filepath)>1:\n",
    "    print(\"There's too many .zip files. There should be only 1\")\n",
    "\n",
    "zfile = ZipFile(zip_filepath[0])\n",
    "zfile.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a tf.data.Dataset \n",
    "\n",
    "Here we are creating the `tf.data` pipeline that executes the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image dataset contains: <MapDataset element_spec=TensorSpec(shape=(64, 64, 3), dtype=tf.float32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "# Section 3.2\n",
    "# Code listing 3.5\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session() # Making sure we are clearing out the TensorFlow graph\n",
    "\n",
    "# Read the CSV file with TensorFlow\n",
    "# The os.path.sep at the end is important for the get_image function\n",
    "data_dir = os.path.join('data', 'flower_images', 'flower_images') + os.path.sep\n",
    "assert os.path.exists(data_dir)\n",
    "csv_ds = tf.data.experimental.CsvDataset(\n",
    "    os.path.join(data_dir,'flower_labels.csv') , (\"\",-1), header=True\n",
    ")\n",
    "# Separate the image names and labels to two separate sets\n",
    "fname_ds = csv_ds.map(lambda a,b: a)\n",
    "label_ds = csv_ds.map(lambda a,b: b)\n",
    "\n",
    "def get_image(file_path):\n",
    "    \n",
    "    img = tf.io.read_file(data_dir + file_path)\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [64, 64])\n",
    "\n",
    "# Get the images by running get_image across all the filenames\n",
    "image_ds = fname_ds.map(get_image)\n",
    "print(\"The image dataset contains: {}\".format(image_ds))\n",
    "# Create onehot encoded labels from label data\n",
    "label_ds = label_ds.map(lambda x: tf.one_hot(x, depth=10))\n",
    "# Zip the images and labels together\n",
    "data_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "\n",
    "# Shuffle the data so that we get a mix of labels in every batch\n",
    "data_ds = data_ds.shuffle(buffer_size= 20)\n",
    "# Define a batch of size 5 \n",
    "data_ds = data_ds.batch(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(5, 64, 64, 3), dtype=float32, numpy=\n",
      "array([[[[0.02941177, 0.02941177, 0.02745098],\n",
      "         [0.04117648, 0.05      , 0.03921569],\n",
      "         [0.14019608, 0.17843139, 0.11176471],\n",
      "         ...,\n",
      "         [0.0254902 , 0.0254902 , 0.0254902 ],\n",
      "         [0.0254902 , 0.0254902 , 0.0254902 ],\n",
      "         [0.02450981, 0.02450981, 0.02450981]],\n",
      "\n",
      "        [[0.04411765, 0.04901961, 0.0382353 ],\n",
      "         [0.13333334, 0.16960785, 0.10490197],\n",
      "         [0.19901963, 0.2735294 , 0.15490197],\n",
      "         ...,\n",
      "         [0.0254902 , 0.0254902 , 0.0254902 ],\n",
      "         [0.02647059, 0.02647059, 0.02647059],\n",
      "         [0.02647059, 0.02745098, 0.0254902 ]],\n",
      "\n",
      "        [[0.12254903, 0.1637255 , 0.09509805],\n",
      "         [0.20686276, 0.2901961 , 0.15490197],\n",
      "         [0.1892157 , 0.27549022, 0.14803922],\n",
      "         ...,\n",
      "         [0.02843137, 0.02941177, 0.02843137],\n",
      "         [0.02843137, 0.03333334, 0.02843137],\n",
      "         [0.0382353 , 0.04411765, 0.03431373]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.88039225, 0.6186274 , 0.72745097],\n",
      "         [0.71274513, 0.47058827, 0.6009804 ],\n",
      "         [0.5254902 , 0.3156863 , 0.44705886],\n",
      "         ...,\n",
      "         [0.05686275, 0.07941177, 0.04705883],\n",
      "         [0.04901961, 0.05784314, 0.04019608],\n",
      "         [0.02450981, 0.0254902 , 0.0254902 ]],\n",
      "\n",
      "        [[0.9235295 , 0.7156863 , 0.75686276],\n",
      "         [0.60882354, 0.3892157 , 0.37549022],\n",
      "         [0.68921566, 0.50784314, 0.58921576],\n",
      "         ...,\n",
      "         [0.06862745, 0.09411766, 0.05392157],\n",
      "         [0.03039216, 0.03529412, 0.02941177],\n",
      "         [0.02450981, 0.02352941, 0.02450981]],\n",
      "\n",
      "        [[0.8264707 , 0.6578431 , 0.7029412 ],\n",
      "         [0.51176476, 0.46078432, 0.43529415],\n",
      "         [0.29705885, 0.35000002, 0.22941178],\n",
      "         ...,\n",
      "         [0.07352941, 0.09509805, 0.05882353],\n",
      "         [0.02352941, 0.02254902, 0.02450981],\n",
      "         [0.0254902 , 0.0254902 , 0.0254902 ]]],\n",
      "\n",
      "\n",
      "       [[[0.23039217, 0.34509805, 0.13235295],\n",
      "         [0.21274512, 0.3       , 0.12450981],\n",
      "         [0.17843139, 0.19901963, 0.10294119],\n",
      "         ...,\n",
      "         [0.37843138, 0.42549023, 0.3137255 ],\n",
      "         [0.29901963, 0.3480392 , 0.23333335],\n",
      "         [0.18039218, 0.2137255 , 0.14313726]],\n",
      "\n",
      "        [[0.28627452, 0.33333334, 0.15882353],\n",
      "         [0.3127451 , 0.3686275 , 0.18823531],\n",
      "         [0.2764706 , 0.33823532, 0.16078433],\n",
      "         ...,\n",
      "         [0.21960786, 0.25980395, 0.16960785],\n",
      "         [0.21666668, 0.24215688, 0.16078433],\n",
      "         [0.15392157, 0.18431374, 0.12058824]],\n",
      "\n",
      "        [[0.582353  , 0.5882353 , 0.3519608 ],\n",
      "         [0.532353  , 0.52254903, 0.28235295],\n",
      "         [0.52156866, 0.5029412 , 0.29509807],\n",
      "         ...,\n",
      "         [0.15980393, 0.1872549 , 0.1254902 ],\n",
      "         [0.1382353 , 0.16568628, 0.10784315],\n",
      "         [0.13627452, 0.15686275, 0.10392158]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.12156864, 0.08627452, 0.09509805],\n",
      "         [0.14705883, 0.07156863, 0.13333334],\n",
      "         [0.14705883, 0.07058824, 0.13529412],\n",
      "         ...,\n",
      "         [0.7578432 , 0.76274514, 0.7578432 ],\n",
      "         [0.6990197 , 0.70686275, 0.7019608 ],\n",
      "         [0.6147059 , 0.62254906, 0.6186275 ]],\n",
      "\n",
      "        [[0.13725491, 0.07450981, 0.11764707],\n",
      "         [0.18529412, 0.07647059, 0.16078432],\n",
      "         [0.1627451 , 0.06960785, 0.14509805],\n",
      "         ...,\n",
      "         [0.7313726 , 0.7294118 , 0.7303922 ],\n",
      "         [0.75098044, 0.75      , 0.7490196 ],\n",
      "         [0.69705886, 0.704902  , 0.7029412 ]],\n",
      "\n",
      "        [[0.24901962, 0.10490197, 0.20784315],\n",
      "         [0.23431374, 0.09607844, 0.1901961 ],\n",
      "         [0.17647061, 0.08039216, 0.1509804 ],\n",
      "         ...,\n",
      "         [0.6205883 , 0.6186275 , 0.6245098 ],\n",
      "         [0.70882356, 0.7019608 , 0.70490193],\n",
      "         [0.73627454, 0.73529416, 0.73627454]]],\n",
      "\n",
      "\n",
      "       [[[0.20490198, 0.2627451 , 0.1392157 ],\n",
      "         [0.22352943, 0.26568627, 0.14901961],\n",
      "         [0.20588237, 0.19803923, 0.17058824],\n",
      "         ...,\n",
      "         [0.31078434, 0.49411768, 0.18137255],\n",
      "         [0.78627455, 0.859804  , 0.77450985],\n",
      "         [0.8235295 , 0.7872549 , 0.82156867]],\n",
      "\n",
      "        [[0.21666668, 0.24509805, 0.17254904],\n",
      "         [0.27254903, 0.33333334, 0.2019608 ],\n",
      "         [0.22549021, 0.24215688, 0.17156863],\n",
      "         ...,\n",
      "         [0.377451  , 0.43921572, 0.30784315],\n",
      "         [0.8000001 , 0.82549024, 0.7754903 ],\n",
      "         [0.66176474, 0.6568628 , 0.6333333 ]],\n",
      "\n",
      "        [[0.22352943, 0.23431374, 0.20098041],\n",
      "         [0.3127451 , 0.3892157 , 0.25      ],\n",
      "         [0.25      , 0.3137255 , 0.1627451 ],\n",
      "         ...,\n",
      "         [0.4431373 , 0.41274512, 0.39313728],\n",
      "         [0.57058823, 0.57549024, 0.5382353 ],\n",
      "         [0.43137258, 0.4156863 , 0.37352943]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.81470597, 0.82549024, 0.7970589 ],\n",
      "         [0.8343138 , 0.84411776, 0.81568635],\n",
      "         [0.84215695, 0.86372554, 0.8235295 ],\n",
      "         ...,\n",
      "         [0.2137255 , 0.34411764, 0.17647061],\n",
      "         [0.23235296, 0.3480392 , 0.19313727],\n",
      "         [0.24509805, 0.36078432, 0.21568629]],\n",
      "\n",
      "        [[0.79117656, 0.80980396, 0.7460785 ],\n",
      "         [0.782353  , 0.80392164, 0.7372549 ],\n",
      "         [0.69803923, 0.7421569 , 0.6352942 ],\n",
      "         ...,\n",
      "         [0.21568629, 0.34607846, 0.18431374],\n",
      "         [0.21568629, 0.34705883, 0.19411767],\n",
      "         [0.2264706 , 0.3529412 , 0.20000002]],\n",
      "\n",
      "        [[0.85686284, 0.86862755, 0.84901965],\n",
      "         [0.85      , 0.86372554, 0.83725494],\n",
      "         [0.6754902 , 0.7539216 , 0.5911765 ],\n",
      "         ...,\n",
      "         [0.15980393, 0.28921568, 0.13137257],\n",
      "         [0.23333335, 0.34509805, 0.20000002],\n",
      "         [0.22941178, 0.34411764, 0.21078433]]],\n",
      "\n",
      "\n",
      "       [[[0.7568628 , 0.19215688, 0.29607844],\n",
      "         [0.7686275 , 0.2392157 , 0.3421569 ],\n",
      "         [0.76274514, 0.227451  , 0.32156864],\n",
      "         ...,\n",
      "         [0.02745098, 0.02745098, 0.02647059],\n",
      "         [0.02352941, 0.02352941, 0.02352941],\n",
      "         [0.03039216, 0.03039216, 0.03039216]],\n",
      "\n",
      "        [[0.7009804 , 0.15784314, 0.23039217],\n",
      "         [0.659804  , 0.13039216, 0.1872549 ],\n",
      "         [0.6166667 , 0.13137256, 0.18039218],\n",
      "         ...,\n",
      "         [0.04313726, 0.04411765, 0.03921569],\n",
      "         [0.03235294, 0.03333334, 0.03333334],\n",
      "         [0.0254902 , 0.0254902 , 0.0254902 ]],\n",
      "\n",
      "        [[0.52450985, 0.11274511, 0.1892157 ],\n",
      "         [0.5352942 , 0.09803922, 0.2009804 ],\n",
      "         [0.5676471 , 0.12058824, 0.24117649],\n",
      "         ...,\n",
      "         [0.04901961, 0.05490196, 0.04803922],\n",
      "         [0.04117647, 0.04313726, 0.04215687],\n",
      "         [0.03235294, 0.03235294, 0.03235294]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.03039216, 0.02941177, 0.02647059],\n",
      "         [0.0254902 , 0.02647059, 0.0254902 ],\n",
      "         [0.03921569, 0.03137255, 0.02745098],\n",
      "         ...,\n",
      "         [0.6696079 , 0.18725492, 0.227451  ],\n",
      "         [0.91176474, 0.60784316, 0.6313726 ],\n",
      "         [0.9245099 , 0.5921569 , 0.6392157 ]],\n",
      "\n",
      "        [[0.02745098, 0.02745098, 0.02352941],\n",
      "         [0.02352941, 0.02352941, 0.02352941],\n",
      "         [0.01764706, 0.01862745, 0.02058824],\n",
      "         ...,\n",
      "         [0.7401961 , 0.33039218, 0.33823532],\n",
      "         [0.89607847, 0.5862745 , 0.6186274 ],\n",
      "         [0.9843138 , 0.61078435, 0.68039215]],\n",
      "\n",
      "        [[0.03333334, 0.03333334, 0.02941177],\n",
      "         [0.02745098, 0.02745098, 0.02647059],\n",
      "         [0.02450981, 0.02254902, 0.02254902],\n",
      "         ...,\n",
      "         [0.82450986, 0.46764708, 0.4647059 ],\n",
      "         [0.9754902 , 0.70000005, 0.7450981 ],\n",
      "         [0.95882356, 0.62058824, 0.7254902 ]]],\n",
      "\n",
      "\n",
      "       [[[0.14509805, 0.19117649, 0.11764707],\n",
      "         [0.15196079, 0.20000002, 0.11960785],\n",
      "         [0.15196079, 0.20098041, 0.11568628],\n",
      "         ...,\n",
      "         [0.17058825, 0.14705884, 0.14117648],\n",
      "         [0.18039218, 0.17843139, 0.1529412 ],\n",
      "         [0.22843137, 0.2245098 , 0.18039218]],\n",
      "\n",
      "        [[0.14019608, 0.18529412, 0.11176471],\n",
      "         [0.14607844, 0.18823531, 0.11470589],\n",
      "         [0.15      , 0.2019608 , 0.11568628],\n",
      "         ...,\n",
      "         [0.23235296, 0.20098041, 0.19509806],\n",
      "         [0.36176473, 0.327451  , 0.2911765 ],\n",
      "         [0.37058824, 0.34411764, 0.29215688]],\n",
      "\n",
      "        [[0.15196079, 0.19509806, 0.12352942],\n",
      "         [0.14215687, 0.18235296, 0.10882354],\n",
      "         [0.14117648, 0.18333334, 0.10980393],\n",
      "         ...,\n",
      "         [0.09901962, 0.10686275, 0.07941177],\n",
      "         [0.12058824, 0.12254903, 0.09215687],\n",
      "         [0.16862747, 0.18333334, 0.13333334]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.27450982, 0.36176473, 0.21764708],\n",
      "         [0.4921569 , 0.59313726, 0.41470593],\n",
      "         [0.48333335, 0.59803927, 0.4039216 ],\n",
      "         ...,\n",
      "         [0.10000001, 0.07156864, 0.12156864],\n",
      "         [0.03627452, 0.03137255, 0.03627452],\n",
      "         [0.01470588, 0.01470588, 0.01470588]],\n",
      "\n",
      "        [[0.4019608 , 0.504902  , 0.30686277],\n",
      "         [0.42352945, 0.5441177 , 0.3421569 ],\n",
      "         [0.43235296, 0.5539216 , 0.35686275],\n",
      "         ...,\n",
      "         [0.03725491, 0.04509804, 0.03137255],\n",
      "         [0.0254902 , 0.0254902 , 0.02450981],\n",
      "         [0.0127451 , 0.0127451 , 0.01176471]],\n",
      "\n",
      "        [[0.40294123, 0.52156866, 0.33137256],\n",
      "         [0.42254907, 0.5401961 , 0.34607846],\n",
      "         [0.4637255 , 0.5833334 , 0.38627455],\n",
      "         ...,\n",
      "         [0.04313726, 0.04607844, 0.04117648],\n",
      "         [0.03333334, 0.03921569, 0.03137255],\n",
      "         [0.01960785, 0.01960785, 0.01960785]]]], dtype=float32)>, <tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
      "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the data to see what it contains\n",
    "for item in data_ds:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and training a model\n",
    "\n",
    "Here we are defining a simple Convolution Neural Network (CNN) model to train it on the image data we just retrieved. You don't have to worry about the technical details of CNNs right now. We will discuss them in detail in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 18:36:00.740818: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8400\n",
      "2022-07-29 18:36:02.055071: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.194, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 3s 9ms/step - loss: 3.1753 - acc: 0.2810\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 1.0638 - acc: 0.7000\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5444 - acc: 0.8762\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2067 - acc: 0.9429\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0588 - acc: 0.9952\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0458 - acc: 0.9905\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0412 - acc: 0.9952\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0167 - acc: 0.9952\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0150 - acc: 0.9952\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45778eb430>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Defining a Convolution neural network for you to train for the flowers data\n",
    "# We will discuss convolution neural networks in more detail later\n",
    "model = Sequential([\n",
    "    Conv2D(64,(5,5), activation='relu', input_shape=(64,64,3)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Training the model with the tf.data pipeline\n",
    "model.fit(data_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras data generators to retrieve data\n",
    "\n",
    "Instead of `tf.data` API let us use the Keras `ImageDataGenerator` to retrieve the data. As you can see, the `ImageDataGenerator` involves much less code than the using the `tf.data` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210 validated image filenames.\n",
      "(array([[[[  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,  74.,   0.],\n",
      "         [ 15.,  60.,   0.],\n",
      "         [ 65.,  37.,   0.]],\n",
      "\n",
      "        [[ 11.,  11.,  11.],\n",
      "         [ 11.,  11.,  11.],\n",
      "         [ 25.,  23.,  20.],\n",
      "         ...,\n",
      "         [ 62., 109.,  50.],\n",
      "         [ 63., 113.,  52.],\n",
      "         [ 65.,  52.,  45.]],\n",
      "\n",
      "        [[ 13.,  14.,  13.],\n",
      "         [ 16.,  17.,  15.],\n",
      "         [ 13.,  15.,  13.],\n",
      "         ...,\n",
      "         [ 61., 109.,  48.],\n",
      "         [ 66., 114.,  52.],\n",
      "         [ 71.,  86.,  51.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[100.,  83.,  72.],\n",
      "         [104.,  89.,  75.],\n",
      "         [ 93.,  80.,  66.],\n",
      "         ...,\n",
      "         [ 89.,  83.,  71.],\n",
      "         [ 94.,  81.,  67.],\n",
      "         [ 14.,  17.,  18.]],\n",
      "\n",
      "        [[ 92.,  80.,  64.],\n",
      "         [115., 101.,  85.],\n",
      "         [ 68.,  61.,  53.],\n",
      "         ...,\n",
      "         [ 80.,  71.,  57.],\n",
      "         [ 75.,  67.,  56.],\n",
      "         [ 56.,  49.,  44.]],\n",
      "\n",
      "        [[ 49.,  42.,  35.],\n",
      "         [ 82.,  72.,  60.],\n",
      "         [125., 109.,  90.],\n",
      "         ...,\n",
      "         [124., 109.,  92.],\n",
      "         [105.,  93.,  76.],\n",
      "         [113.,  97.,  81.]]],\n",
      "\n",
      "\n",
      "       [[[ 47.,  84.,  36.],\n",
      "         [ 27.,  59.,  24.],\n",
      "         [ 56.,  92.,  55.],\n",
      "         ...,\n",
      "         [ 98., 130., 103.],\n",
      "         [ 95., 128.,  96.],\n",
      "         [ 85., 119.,  79.]],\n",
      "\n",
      "        [[ 47.,  82.,  33.],\n",
      "         [ 49.,  83.,  38.],\n",
      "         [ 50.,  86.,  43.],\n",
      "         ...,\n",
      "         [100., 135., 104.],\n",
      "         [ 93., 126.,  92.],\n",
      "         [ 83., 118.,  84.]],\n",
      "\n",
      "        [[ 91., 128., 101.],\n",
      "         [ 47.,  87.,  39.],\n",
      "         [ 62., 102.,  55.],\n",
      "         ...,\n",
      "         [ 95., 131.,  96.],\n",
      "         [ 99., 135., 101.],\n",
      "         [ 86., 123.,  89.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 82., 123.,  80.],\n",
      "         [109., 156., 120.],\n",
      "         [115., 165., 119.],\n",
      "         ...,\n",
      "         [103., 136., 107.],\n",
      "         [ 84.,  97.,  79.],\n",
      "         [ 31.,  61.,  32.]],\n",
      "\n",
      "        [[ 92., 140.,  86.],\n",
      "         [ 97., 150.,  99.],\n",
      "         [105., 156., 114.],\n",
      "         ...,\n",
      "         [ 84., 107.,  94.],\n",
      "         [ 41.,  63.,  40.],\n",
      "         [ 34.,  55.,  31.]],\n",
      "\n",
      "        [[ 93., 147.,  71.],\n",
      "         [100., 145.,  92.],\n",
      "         [ 90., 138.,  97.],\n",
      "         ...,\n",
      "         [ 59.,  69.,  59.],\n",
      "         [ 13.,  14.,  15.],\n",
      "         [ 34.,  60.,  33.]]],\n",
      "\n",
      "\n",
      "       [[[102., 120.,  39.],\n",
      "         [ 98., 104.,  38.],\n",
      "         [132., 117.,  43.],\n",
      "         ...,\n",
      "         [125.,  31.,  47.],\n",
      "         [119.,  37.,  45.],\n",
      "         [108.,  42.,  42.]],\n",
      "\n",
      "        [[111., 126.,  42.],\n",
      "         [110., 109.,  38.],\n",
      "         [144., 123.,  47.],\n",
      "         ...,\n",
      "         [131.,  52.,  45.],\n",
      "         [126.,  58.,  45.],\n",
      "         [116.,  63.,  42.]],\n",
      "\n",
      "        [[118., 129.,  43.],\n",
      "         [120., 113.,  40.],\n",
      "         [142., 120.,  45.],\n",
      "         ...,\n",
      "         [146.,  76.,  51.],\n",
      "         [140.,  81.,  49.],\n",
      "         [129.,  83.,  45.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[172., 181.,  74.],\n",
      "         [175., 186.,  65.],\n",
      "         [172., 179.,  57.],\n",
      "         ...,\n",
      "         [136., 172.,  66.],\n",
      "         [139., 170.,  67.],\n",
      "         [139., 166.,  66.]],\n",
      "\n",
      "        [[173., 181.,  72.],\n",
      "         [174., 181.,  61.],\n",
      "         [154., 165.,  53.],\n",
      "         ...,\n",
      "         [139., 174.,  67.],\n",
      "         [139., 169.,  67.],\n",
      "         [137., 163.,  65.]],\n",
      "\n",
      "        [[176., 183.,  69.],\n",
      "         [178., 183.,  63.],\n",
      "         [135., 148.,  50.],\n",
      "         ...,\n",
      "         [138., 173.,  66.],\n",
      "         [138., 168.,  65.],\n",
      "         [134., 158.,  64.]]],\n",
      "\n",
      "\n",
      "       [[[ 16.,  21.,  15.],\n",
      "         [ 15.,  27.,  21.],\n",
      "         [ 73.,  96.,  62.],\n",
      "         ...,\n",
      "         [  8.,  13.,   4.],\n",
      "         [ 41.,  57.,  39.],\n",
      "         [ 20.,  30.,  19.]],\n",
      "\n",
      "        [[ 17.,  23.,  18.],\n",
      "         [  5.,  13.,  16.],\n",
      "         [ 98., 123.,  52.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.],\n",
      "         [ 69.,  91.,  68.],\n",
      "         [ 24.,  34.,  23.]],\n",
      "\n",
      "        [[ 19.,  24.,  19.],\n",
      "         [ 17.,  22.,  14.],\n",
      "         [ 63.,  73.,  22.],\n",
      "         ...,\n",
      "         [  7.,   0.,   8.],\n",
      "         [ 83., 114.,  80.],\n",
      "         [ 21.,  31.,  18.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 19.,  25.,  16.],\n",
      "         [ 21.,  28.,  16.],\n",
      "         [ 22.,  29.,  17.],\n",
      "         ...,\n",
      "         [ 20.,  25.,  18.],\n",
      "         [ 18.,  20.,  15.],\n",
      "         [ 14.,  16.,  13.]],\n",
      "\n",
      "        [[ 18.,  23.,  16.],\n",
      "         [ 19.,  27.,  17.],\n",
      "         [ 22.,  30.,  16.],\n",
      "         ...,\n",
      "         [ 19.,  23.,  16.],\n",
      "         [ 16.,  19.,  15.],\n",
      "         [ 16.,  18.,  13.]],\n",
      "\n",
      "        [[ 18.,  21.,  14.],\n",
      "         [ 20.,  26.,  16.],\n",
      "         [ 22.,  30.,  16.],\n",
      "         ...,\n",
      "         [ 16.,  18.,  13.],\n",
      "         [ 14.,  17.,  13.],\n",
      "         [ 17.,  19.,  14.]]],\n",
      "\n",
      "\n",
      "       [[[ 20.,  25.,  16.],\n",
      "         [ 28.,  38.,  24.],\n",
      "         [ 18.,  20.,  13.],\n",
      "         ...,\n",
      "         [ 14.,  17.,  12.],\n",
      "         [  9.,  11.,   8.],\n",
      "         [ 10.,  12.,  11.]],\n",
      "\n",
      "        [[ 30.,  41.,  25.],\n",
      "         [ 16.,  21.,  13.],\n",
      "         [ 23.,  31.,  16.],\n",
      "         ...,\n",
      "         [ 16.,  19.,  13.],\n",
      "         [ 12.,  13.,  10.],\n",
      "         [  8.,   9.,   8.]],\n",
      "\n",
      "        [[ 21.,  29.,  17.],\n",
      "         [ 23.,  31.,  18.],\n",
      "         [ 23.,  32.,  17.],\n",
      "         ...,\n",
      "         [ 14.,  16.,  12.],\n",
      "         [ 12.,  14.,  10.],\n",
      "         [ 12.,  14.,  10.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 31.,  45.,  21.],\n",
      "         [ 27.,  41.,  17.],\n",
      "         [ 19.,  23.,  14.],\n",
      "         ...,\n",
      "         [ 25.,  33.,  21.],\n",
      "         [ 16.,  19.,  13.],\n",
      "         [ 16.,  20.,  13.]],\n",
      "\n",
      "        [[ 29.,  44.,  19.],\n",
      "         [ 31.,  46.,  22.],\n",
      "         [ 32.,  44.,  19.],\n",
      "         ...,\n",
      "         [ 20.,  27.,  16.],\n",
      "         [ 13.,  18.,  12.],\n",
      "         [ 17.,  21.,  15.]],\n",
      "\n",
      "        [[ 25.,  37.,  19.],\n",
      "         [ 26.,  40.,  19.],\n",
      "         [ 33.,  44.,  24.],\n",
      "         ...,\n",
      "         [ 16.,  19.,  14.],\n",
      "         [ 17.,  19.,  14.],\n",
      "         [ 15.,  18.,  12.]]]], dtype=float32), array([8, 8, 9, 3, 4]))\n"
     ]
    }
   ],
   "source": [
    "# Section 3.2\n",
    "# Code listing 3.6\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = os.path.join('data','flower_images', 'flower_images')\n",
    "\n",
    "# Defining an image data generator provided in Keras\n",
    "img_gen = ImageDataGenerator()\n",
    "\n",
    "# Reading the CSV files containing filenames and labels\n",
    "labels_df = pd.read_csv(os.path.join(data_dir, 'flower_labels.csv'), header=0)\n",
    "\n",
    "# Generating data using the flow_from_dataframe function\n",
    "gen_iter = img_gen.flow_from_dataframe(\n",
    "    dataframe=labels_df, directory=data_dir, x_col='file', y_col='label', class_mode='raw', batch_size=5, target_size=(64,64))\n",
    "\n",
    "# Iterating through the data\n",
    "for item in gen_iter:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `tensorflow-datasets` library\n",
    "\n",
    "Here we will use the `tensorflow-datasets` package. It is a curated list of popular datasets available for machine learning projects. With this package you can download a dataset in a single line. This means you don't have to worry about downloading/extracting/formatting data manually. All of that will be already done when you import data using the `tensorflow-datasets` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists the available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 18:36:08.837006: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'accentdb',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'ag_news_subset',\n",
       " 'ai2_arc',\n",
       " 'ai2_arc_with_ir',\n",
       " 'amazon_us_reviews',\n",
       " 'anli',\n",
       " 'answer_equivalence',\n",
       " 'arc',\n",
       " 'asqa',\n",
       " 'asset',\n",
       " 'assin2',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bccd',\n",
       " 'beans',\n",
       " 'bee_dataset',\n",
       " 'beir',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'ble_wind_field',\n",
       " 'blimp',\n",
       " 'booksum',\n",
       " 'bool_q',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cardiotox',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'cherry_blossoms',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'clic',\n",
       " 'clinc_oos',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco_captions',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'common_voice',\n",
       " 'coqa',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'covid19',\n",
       " 'covid19sum',\n",
       " 'crema_d',\n",
       " 'criteo',\n",
       " 'cs_restaurants',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'd4rl_adroit_door',\n",
       " 'd4rl_adroit_hammer',\n",
       " 'd4rl_adroit_pen',\n",
       " 'd4rl_adroit_relocate',\n",
       " 'd4rl_antmaze',\n",
       " 'd4rl_mujoco_ant',\n",
       " 'd4rl_mujoco_halfcheetah',\n",
       " 'd4rl_mujoco_hopper',\n",
       " 'd4rl_mujoco_walker2d',\n",
       " 'dart',\n",
       " 'davis',\n",
       " 'deep1b',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dementiabank',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'diamonds',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'doc_nli',\n",
       " 'dolphin_number_word',\n",
       " 'domainnet',\n",
       " 'downsampled_imagenet',\n",
       " 'drop',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'e2e_cleaned',\n",
       " 'efron_morris75',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'forest_fires',\n",
       " 'fuss',\n",
       " 'gap',\n",
       " 'geirhos_conflict_stimuli',\n",
       " 'gem',\n",
       " 'genomics_ood',\n",
       " 'german_credit_numeric',\n",
       " 'gigaword',\n",
       " 'glove100_angular',\n",
       " 'glue',\n",
       " 'goemotions',\n",
       " 'gov_report',\n",
       " 'gpt3',\n",
       " 'gref',\n",
       " 'groove',\n",
       " 'grounded_scan',\n",
       " 'gsm8k',\n",
       " 'gtzan',\n",
       " 'gtzan_music_speech',\n",
       " 'hellaswag',\n",
       " 'higgs',\n",
       " 'hillstrom',\n",
       " 'horses_or_humans',\n",
       " 'howell',\n",
       " 'i_naturalist2017',\n",
       " 'i_naturalist2018',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet2012_fewshot',\n",
       " 'imagenet2012_multilabel',\n",
       " 'imagenet2012_real',\n",
       " 'imagenet2012_subset',\n",
       " 'imagenet_a',\n",
       " 'imagenet_lt',\n",
       " 'imagenet_r',\n",
       " 'imagenet_resized',\n",
       " 'imagenet_sketch',\n",
       " 'imagenet_v2',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'irc_disentanglement',\n",
       " 'iris',\n",
       " 'istella',\n",
       " 'kddcup99',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lambada',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'ljspeech',\n",
       " 'lm1b',\n",
       " 'locomotion',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'lvis',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'math_qa',\n",
       " 'mctaco',\n",
       " 'media_sum',\n",
       " 'mlqa',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_lens',\n",
       " 'movie_rationales',\n",
       " 'movielens',\n",
       " 'moving_mnist',\n",
       " 'mrqa',\n",
       " 'mslr_web',\n",
       " 'mt_opt',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_questions',\n",
       " 'natural_questions_open',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'nyu_depth_v2',\n",
       " 'ogbg_molpcba',\n",
       " 'omniglot',\n",
       " 'open_images_challenge2019_detection',\n",
       " 'open_images_v4',\n",
       " 'openbookqa',\n",
       " 'opinion_abstracts',\n",
       " 'opinosis',\n",
       " 'opus',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'pass',\n",
       " 'patch_camelyon',\n",
       " 'paws_wiki',\n",
       " 'paws_x_wiki',\n",
       " 'penguins',\n",
       " 'pet_finder',\n",
       " 'pg19',\n",
       " 'piqa',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'protein_net',\n",
       " 'qa4mre',\n",
       " 'qasc',\n",
       " 'quac',\n",
       " 'quality',\n",
       " 'quickdraw_bitmap',\n",
       " 'race',\n",
       " 'radon',\n",
       " 'reddit',\n",
       " 'reddit_disentanglement',\n",
       " 'reddit_tifu',\n",
       " 'ref_coco',\n",
       " 'resisc45',\n",
       " 'rlu_atari',\n",
       " 'rlu_atari_checkpoints',\n",
       " 'rlu_atari_checkpoints_ordered',\n",
       " 'rlu_control_suite',\n",
       " 'rlu_dmlab_explore_object_rewards_few',\n",
       " 'rlu_dmlab_explore_object_rewards_many',\n",
       " 'rlu_dmlab_rooms_select_nonmatching_object',\n",
       " 'rlu_dmlab_rooms_watermaze',\n",
       " 'rlu_dmlab_seekavoid_arena01',\n",
       " 'rlu_locomotion',\n",
       " 'rlu_rwrl',\n",
       " 'robomimic_ph',\n",
       " 'robonet',\n",
       " 'robosuite_panda_pick_place_can',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 's3o4d',\n",
       " 'salient_span_wikipedia',\n",
       " 'samsum',\n",
       " 'savee',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'schema_guided_dialogue',\n",
       " 'sci_tail',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'scrolls',\n",
       " 'sentiment140',\n",
       " 'shapes3d',\n",
       " 'sift1m',\n",
       " 'simpte',\n",
       " 'siscore',\n",
       " 'smallnorb',\n",
       " 'smartwatch_gestures',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'spoken_digit',\n",
       " 'squad',\n",
       " 'squad_question_generation',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'star_cfq',\n",
       " 'starcraft_video',\n",
       " 'stl10',\n",
       " 'story_cloze',\n",
       " 'summscreen',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'symmetric_solids',\n",
       " 'tao',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tedlium',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tydi_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'unified_qa',\n",
       " 'vctk',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'voxceleb',\n",
       " 'voxforge',\n",
       " 'waymo_open_dataset',\n",
       " 'web_graph',\n",
       " 'web_nlg',\n",
       " 'web_questions',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wiki_auto',\n",
       " 'wiki_bio',\n",
       " 'wiki_dialog',\n",
       " 'wiki_table_questions',\n",
       " 'wiki_table_text',\n",
       " 'wikiann',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikipedia_toxicity_subtypes',\n",
       " 'wine_quality',\n",
       " 'winogrande',\n",
       " 'wit',\n",
       " 'wit_kaggle',\n",
       " 'wmt13_translate',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'wordnet',\n",
       " 'wsc273',\n",
       " 'xnli',\n",
       " 'xquad',\n",
       " 'xsum',\n",
       " 'xtreme_pawsx',\n",
       " 'xtreme_s',\n",
       " 'xtreme_xnli',\n",
       " 'yelp_polarity_reviews',\n",
       " 'yes_no',\n",
       " 'youtube_vis',\n",
       " 'huggingface:acronym_identification',\n",
       " 'huggingface:ade_corpus_v2',\n",
       " 'huggingface:adv_glue',\n",
       " 'huggingface:adversarial_qa',\n",
       " 'huggingface:aeslc',\n",
       " 'huggingface:afrikaans_ner_corpus',\n",
       " 'huggingface:ag_news',\n",
       " 'huggingface:ai2_arc',\n",
       " 'huggingface:air_dialogue',\n",
       " 'huggingface:ajgt_twitter_ar',\n",
       " 'huggingface:allegro_reviews',\n",
       " 'huggingface:allocine',\n",
       " 'huggingface:alt',\n",
       " 'huggingface:amazon_polarity',\n",
       " 'huggingface:amazon_reviews_multi',\n",
       " 'huggingface:amazon_us_reviews',\n",
       " 'huggingface:ambig_qa',\n",
       " 'huggingface:americas_nli',\n",
       " 'huggingface:ami',\n",
       " 'huggingface:amttl',\n",
       " 'huggingface:anli',\n",
       " 'huggingface:app_reviews',\n",
       " 'huggingface:aqua_rat',\n",
       " 'huggingface:aquamuse',\n",
       " 'huggingface:ar_cov19',\n",
       " 'huggingface:ar_res_reviews',\n",
       " 'huggingface:ar_sarcasm',\n",
       " 'huggingface:arabic_billion_words',\n",
       " 'huggingface:arabic_pos_dialect',\n",
       " 'huggingface:arabic_speech_corpus',\n",
       " 'huggingface:arcd',\n",
       " 'huggingface:arsentd_lev',\n",
       " 'huggingface:art',\n",
       " 'huggingface:arxiv_dataset',\n",
       " 'huggingface:ascent_kb',\n",
       " 'huggingface:aslg_pc12',\n",
       " 'huggingface:asnq',\n",
       " 'huggingface:asset',\n",
       " 'huggingface:assin',\n",
       " 'huggingface:assin2',\n",
       " 'huggingface:atomic',\n",
       " 'huggingface:autshumato',\n",
       " 'huggingface:babi_qa',\n",
       " 'huggingface:banking77',\n",
       " 'huggingface:bbaw_egyptian',\n",
       " 'huggingface:bbc_hindi_nli',\n",
       " 'huggingface:bc2gm_corpus',\n",
       " 'huggingface:beans',\n",
       " 'huggingface:best2009',\n",
       " 'huggingface:bianet',\n",
       " 'huggingface:bible_para',\n",
       " 'huggingface:big_patent',\n",
       " 'huggingface:bigbench',\n",
       " 'huggingface:billsum',\n",
       " 'huggingface:bing_coronavirus_query_set',\n",
       " 'huggingface:biomrc',\n",
       " 'huggingface:biosses',\n",
       " 'huggingface:biwi_kinect_head_pose',\n",
       " 'huggingface:blbooks',\n",
       " 'huggingface:blbooksgenre',\n",
       " 'huggingface:blended_skill_talk',\n",
       " 'huggingface:blimp',\n",
       " 'huggingface:blog_authorship_corpus',\n",
       " 'huggingface:bn_hate_speech',\n",
       " 'huggingface:bnl_newspapers',\n",
       " 'huggingface:bookcorpus',\n",
       " 'huggingface:bookcorpusopen',\n",
       " 'huggingface:boolq',\n",
       " 'huggingface:bprec',\n",
       " 'huggingface:break_data',\n",
       " 'huggingface:brwac',\n",
       " 'huggingface:bsd_ja_en',\n",
       " 'huggingface:bswac',\n",
       " 'huggingface:c3',\n",
       " 'huggingface:c4',\n",
       " 'huggingface:cail2018',\n",
       " 'huggingface:caner',\n",
       " 'huggingface:capes',\n",
       " 'huggingface:casino',\n",
       " 'huggingface:catalonia_independence',\n",
       " 'huggingface:cats_vs_dogs',\n",
       " 'huggingface:cawac',\n",
       " 'huggingface:cbt',\n",
       " 'huggingface:cc100',\n",
       " 'huggingface:cc_news',\n",
       " 'huggingface:ccaligned_multilingual',\n",
       " 'huggingface:cdsc',\n",
       " 'huggingface:cdt',\n",
       " 'huggingface:cedr',\n",
       " 'huggingface:cfq',\n",
       " 'huggingface:chr_en',\n",
       " 'huggingface:cifar10',\n",
       " 'huggingface:cifar100',\n",
       " 'huggingface:circa',\n",
       " 'huggingface:civil_comments',\n",
       " 'huggingface:clickbait_news_bg',\n",
       " 'huggingface:climate_fever',\n",
       " 'huggingface:clinc_oos',\n",
       " 'huggingface:clue',\n",
       " 'huggingface:cmrc2018',\n",
       " 'huggingface:cmu_hinglish_dog',\n",
       " 'huggingface:cnn_dailymail',\n",
       " 'huggingface:coached_conv_pref',\n",
       " 'huggingface:coarse_discourse',\n",
       " 'huggingface:codah',\n",
       " 'huggingface:code_search_net',\n",
       " 'huggingface:code_x_glue_cc_clone_detection_big_clone_bench',\n",
       " 'huggingface:code_x_glue_cc_clone_detection_poj104',\n",
       " 'huggingface:code_x_glue_cc_cloze_testing_all',\n",
       " 'huggingface:code_x_glue_cc_cloze_testing_maxmin',\n",
       " 'huggingface:code_x_glue_cc_code_completion_line',\n",
       " 'huggingface:code_x_glue_cc_code_completion_token',\n",
       " 'huggingface:code_x_glue_cc_code_refinement',\n",
       " 'huggingface:code_x_glue_cc_code_to_code_trans',\n",
       " 'huggingface:code_x_glue_cc_defect_detection',\n",
       " 'huggingface:code_x_glue_ct_code_to_text',\n",
       " 'huggingface:code_x_glue_tc_nl_code_search_adv',\n",
       " 'huggingface:code_x_glue_tc_text_to_code',\n",
       " 'huggingface:code_x_glue_tt_text_to_text',\n",
       " 'huggingface:com_qa',\n",
       " 'huggingface:common_gen',\n",
       " 'huggingface:common_language',\n",
       " 'huggingface:common_voice',\n",
       " 'huggingface:commonsense_qa',\n",
       " 'huggingface:competition_math',\n",
       " 'huggingface:compguesswhat',\n",
       " 'huggingface:conceptnet5',\n",
       " 'huggingface:conceptual_12m',\n",
       " 'huggingface:conceptual_captions',\n",
       " 'huggingface:conll2000',\n",
       " 'huggingface:conll2002',\n",
       " 'huggingface:conll2003',\n",
       " 'huggingface:conll2012_ontonotesv5',\n",
       " 'huggingface:conllpp',\n",
       " 'huggingface:consumer-finance-complaints',\n",
       " 'huggingface:conv_ai',\n",
       " 'huggingface:conv_ai_2',\n",
       " 'huggingface:conv_ai_3',\n",
       " 'huggingface:conv_questions',\n",
       " 'huggingface:coqa',\n",
       " 'huggingface:cord19',\n",
       " 'huggingface:cornell_movie_dialog',\n",
       " 'huggingface:cos_e',\n",
       " 'huggingface:cosmos_qa',\n",
       " 'huggingface:counter',\n",
       " 'huggingface:covid_qa_castorini',\n",
       " 'huggingface:covid_qa_deepset',\n",
       " 'huggingface:covid_qa_ucsd',\n",
       " 'huggingface:covid_tweets_japanese',\n",
       " 'huggingface:covost2',\n",
       " 'huggingface:cppe-5',\n",
       " 'huggingface:craigslist_bargains',\n",
       " 'huggingface:crawl_domain',\n",
       " 'huggingface:crd3',\n",
       " 'huggingface:crime_and_punish',\n",
       " 'huggingface:crows_pairs',\n",
       " 'huggingface:cryptonite',\n",
       " 'huggingface:cs_restaurants',\n",
       " 'huggingface:cuad',\n",
       " 'huggingface:curiosity_dialogs',\n",
       " 'huggingface:daily_dialog',\n",
       " 'huggingface:dane',\n",
       " 'huggingface:danish_political_comments',\n",
       " 'huggingface:dart',\n",
       " 'huggingface:datacommons_factcheck',\n",
       " 'huggingface:dbpedia_14',\n",
       " 'huggingface:dbrd',\n",
       " 'huggingface:deal_or_no_dialog',\n",
       " 'huggingface:definite_pronoun_resolution',\n",
       " 'huggingface:dengue_filipino',\n",
       " 'huggingface:dialog_re',\n",
       " 'huggingface:diplomacy_detection',\n",
       " 'huggingface:disaster_response_messages',\n",
       " 'huggingface:discofuse',\n",
       " 'huggingface:discovery',\n",
       " 'huggingface:disfl_qa',\n",
       " 'huggingface:doc2dial',\n",
       " 'huggingface:docred',\n",
       " 'huggingface:doqa',\n",
       " 'huggingface:dream',\n",
       " 'huggingface:drop',\n",
       " 'huggingface:duorc',\n",
       " 'huggingface:dutch_social',\n",
       " 'huggingface:dyk',\n",
       " 'huggingface:e2e_nlg',\n",
       " 'huggingface:e2e_nlg_cleaned',\n",
       " 'huggingface:ecb',\n",
       " 'huggingface:ecthr_cases',\n",
       " 'huggingface:eduge',\n",
       " 'huggingface:ehealth_kd',\n",
       " 'huggingface:eitb_parcc',\n",
       " 'huggingface:electricity_load_diagrams',\n",
       " 'huggingface:eli5',\n",
       " 'huggingface:eli5_category',\n",
       " 'huggingface:elkarhizketak',\n",
       " 'huggingface:emea',\n",
       " 'huggingface:emo',\n",
       " 'huggingface:emotion',\n",
       " 'huggingface:emotone_ar',\n",
       " 'huggingface:empathetic_dialogues',\n",
       " 'huggingface:enriched_web_nlg',\n",
       " 'huggingface:enwik8',\n",
       " 'huggingface:eraser_multi_rc',\n",
       " 'huggingface:esnli',\n",
       " 'huggingface:eth_py150_open',\n",
       " 'huggingface:ethos',\n",
       " 'huggingface:ett',\n",
       " 'huggingface:eu_regulatory_ir',\n",
       " 'huggingface:eurlex',\n",
       " 'huggingface:euronews',\n",
       " 'huggingface:europa_eac_tm',\n",
       " 'huggingface:europa_ecdc_tm',\n",
       " 'huggingface:europarl_bilingual',\n",
       " 'huggingface:event2Mind',\n",
       " 'huggingface:evidence_infer_treatment',\n",
       " 'huggingface:exams',\n",
       " 'huggingface:factckbr',\n",
       " 'huggingface:fake_news_english',\n",
       " 'huggingface:fake_news_filipino',\n",
       " 'huggingface:farsi_news',\n",
       " 'huggingface:fashion_mnist',\n",
       " 'huggingface:fever',\n",
       " 'huggingface:few_rel',\n",
       " 'huggingface:financial_phrasebank',\n",
       " 'huggingface:finer',\n",
       " 'huggingface:flores',\n",
       " 'huggingface:flue',\n",
       " 'huggingface:food101',\n",
       " 'huggingface:fquad',\n",
       " 'huggingface:freebase_qa',\n",
       " 'huggingface:gap',\n",
       " 'huggingface:gem',\n",
       " 'huggingface:generated_reviews_enth',\n",
       " 'huggingface:generics_kb',\n",
       " 'huggingface:german_legal_entity_recognition',\n",
       " 'huggingface:germaner',\n",
       " 'huggingface:germeval_14',\n",
       " 'huggingface:giga_fren',\n",
       " 'huggingface:gigaword',\n",
       " 'huggingface:glucose',\n",
       " 'huggingface:glue',\n",
       " 'huggingface:gnad10',\n",
       " 'huggingface:go_emotions',\n",
       " 'huggingface:gooaq',\n",
       " 'huggingface:google_wellformed_query',\n",
       " 'huggingface:grail_qa',\n",
       " 'huggingface:great_code',\n",
       " 'huggingface:greek_legal_code',\n",
       " 'huggingface:gsm8k',\n",
       " 'huggingface:guardian_authorship',\n",
       " 'huggingface:gutenberg_time',\n",
       " 'huggingface:hans',\n",
       " 'huggingface:hansards',\n",
       " 'huggingface:hard',\n",
       " 'huggingface:harem',\n",
       " 'huggingface:has_part',\n",
       " 'huggingface:hate_offensive',\n",
       " 'huggingface:hate_speech18',\n",
       " 'huggingface:hate_speech_filipino',\n",
       " 'huggingface:hate_speech_offensive',\n",
       " 'huggingface:hate_speech_pl',\n",
       " 'huggingface:hate_speech_portuguese',\n",
       " 'huggingface:hatexplain',\n",
       " 'huggingface:hausa_voa_ner',\n",
       " 'huggingface:hausa_voa_topics',\n",
       " 'huggingface:hda_nli_hindi',\n",
       " 'huggingface:head_qa',\n",
       " 'huggingface:health_fact',\n",
       " 'huggingface:hebrew_projectbenyehuda',\n",
       " 'huggingface:hebrew_sentiment',\n",
       " 'huggingface:hebrew_this_world',\n",
       " 'huggingface:hellaswag',\n",
       " 'huggingface:hendrycks_test',\n",
       " 'huggingface:hind_encorp',\n",
       " 'huggingface:hindi_discourse',\n",
       " 'huggingface:hippocorpus',\n",
       " 'huggingface:hkcancor',\n",
       " 'huggingface:hlgd',\n",
       " 'huggingface:hope_edi',\n",
       " 'huggingface:hotpot_qa',\n",
       " 'huggingface:hover',\n",
       " 'huggingface:hrenwac_para',\n",
       " 'huggingface:hrwac',\n",
       " 'huggingface:humicroedit',\n",
       " 'huggingface:hybrid_qa',\n",
       " 'huggingface:hyperpartisan_news_detection',\n",
       " 'huggingface:iapp_wiki_qa_squad',\n",
       " 'huggingface:id_clickbait',\n",
       " 'huggingface:id_liputan6',\n",
       " 'huggingface:id_nergrit_corpus',\n",
       " 'huggingface:id_newspapers_2018',\n",
       " 'huggingface:id_panl_bppt',\n",
       " 'huggingface:id_puisi',\n",
       " 'huggingface:igbo_english_machine_translation',\n",
       " 'huggingface:igbo_monolingual',\n",
       " 'huggingface:igbo_ner',\n",
       " 'huggingface:ilist',\n",
       " 'huggingface:imagenet-1k',\n",
       " 'huggingface:imagenet_sketch',\n",
       " 'huggingface:imdb',\n",
       " 'huggingface:imdb_urdu_reviews',\n",
       " 'huggingface:imppres',\n",
       " 'huggingface:indic_glue',\n",
       " 'huggingface:indonli',\n",
       " 'huggingface:indonlu',\n",
       " 'huggingface:inquisitive_qg',\n",
       " 'huggingface:interpress_news_category_tr',\n",
       " 'huggingface:interpress_news_category_tr_lite',\n",
       " 'huggingface:irc_disentangle',\n",
       " 'huggingface:isixhosa_ner_corpus',\n",
       " 'huggingface:isizulu_ner_corpus',\n",
       " 'huggingface:iwslt2017',\n",
       " 'huggingface:jeopardy',\n",
       " 'huggingface:jfleg',\n",
       " 'huggingface:jigsaw_toxicity_pred',\n",
       " 'huggingface:jigsaw_unintended_bias',\n",
       " 'huggingface:jnlpba',\n",
       " 'huggingface:journalists_questions',\n",
       " 'huggingface:kan_hope',\n",
       " 'huggingface:kannada_news',\n",
       " 'huggingface:kd_conv',\n",
       " 'huggingface:kde4',\n",
       " 'huggingface:kelm',\n",
       " 'huggingface:kilt_tasks',\n",
       " 'huggingface:kilt_wikipedia',\n",
       " 'huggingface:kinnews_kirnews',\n",
       " 'huggingface:klue',\n",
       " 'huggingface:kor_3i4k',\n",
       " 'huggingface:kor_hate',\n",
       " 'huggingface:kor_ner',\n",
       " 'huggingface:kor_nli',\n",
       " 'huggingface:kor_nlu',\n",
       " 'huggingface:kor_qpair',\n",
       " 'huggingface:kor_sae',\n",
       " 'huggingface:kor_sarcasm',\n",
       " 'huggingface:labr',\n",
       " 'huggingface:lama',\n",
       " 'huggingface:lambada',\n",
       " 'huggingface:large_spanish_corpus',\n",
       " 'huggingface:laroseda',\n",
       " 'huggingface:lc_quad',\n",
       " 'huggingface:lccc',\n",
       " 'huggingface:lener_br',\n",
       " 'huggingface:lex_glue',\n",
       " 'huggingface:liar',\n",
       " 'huggingface:librispeech_asr',\n",
       " 'huggingface:librispeech_lm',\n",
       " 'huggingface:limit',\n",
       " 'huggingface:lince',\n",
       " 'huggingface:linnaeus',\n",
       " 'huggingface:liveqa',\n",
       " 'huggingface:lj_speech',\n",
       " 'huggingface:lm1b',\n",
       " 'huggingface:lst20',\n",
       " 'huggingface:m_lama',\n",
       " 'huggingface:mac_morpho',\n",
       " 'huggingface:makhzan',\n",
       " 'huggingface:masakhaner',\n",
       " 'huggingface:math_dataset',\n",
       " 'huggingface:math_qa',\n",
       " 'huggingface:matinf',\n",
       " 'huggingface:mbpp',\n",
       " 'huggingface:mc4',\n",
       " 'huggingface:mc_taco',\n",
       " 'huggingface:md_gender_bias',\n",
       " 'huggingface:mdd',\n",
       " 'huggingface:med_hop',\n",
       " 'huggingface:medal',\n",
       " 'huggingface:medical_dialog',\n",
       " 'huggingface:medical_questions_pairs',\n",
       " 'huggingface:medmcqa',\n",
       " 'huggingface:menyo20k_mt',\n",
       " 'huggingface:meta_woz',\n",
       " 'huggingface:metashift',\n",
       " 'huggingface:metooma',\n",
       " 'huggingface:metrec',\n",
       " 'huggingface:miam',\n",
       " 'huggingface:mkb',\n",
       " 'huggingface:mkqa',\n",
       " 'huggingface:mlqa',\n",
       " 'huggingface:mlsum',\n",
       " 'huggingface:mnist',\n",
       " 'huggingface:mocha',\n",
       " 'huggingface:monash_tsf',\n",
       " 'huggingface:moroco',\n",
       " 'huggingface:movie_rationales',\n",
       " 'huggingface:mrqa',\n",
       " 'huggingface:ms_marco',\n",
       " 'huggingface:ms_terms',\n",
       " 'huggingface:msr_genomics_kbcomp',\n",
       " 'huggingface:msr_sqa',\n",
       " 'huggingface:msr_text_compression',\n",
       " 'huggingface:msr_zhen_translation_parity',\n",
       " 'huggingface:msra_ner',\n",
       " 'huggingface:mt_eng_vietnamese',\n",
       " 'huggingface:muchocine',\n",
       " 'huggingface:multi_booked',\n",
       " 'huggingface:multi_eurlex',\n",
       " 'huggingface:multi_news',\n",
       " 'huggingface:multi_nli',\n",
       " 'huggingface:multi_nli_mismatch',\n",
       " 'huggingface:multi_para_crawl',\n",
       " 'huggingface:multi_re_qa',\n",
       " 'huggingface:multi_woz_v22',\n",
       " 'huggingface:multi_x_science_sum',\n",
       " 'huggingface:multidoc2dial',\n",
       " 'huggingface:multilingual_librispeech',\n",
       " 'huggingface:mutual_friends',\n",
       " 'huggingface:mwsc',\n",
       " 'huggingface:myanmar_news',\n",
       " 'huggingface:narrativeqa',\n",
       " 'huggingface:narrativeqa_manual',\n",
       " 'huggingface:natural_questions',\n",
       " 'huggingface:ncbi_disease',\n",
       " 'huggingface:nchlt',\n",
       " 'huggingface:ncslgr',\n",
       " 'huggingface:nell',\n",
       " 'huggingface:neural_code_search',\n",
       " 'huggingface:news_commentary',\n",
       " 'huggingface:newsgroup',\n",
       " 'huggingface:newsph',\n",
       " 'huggingface:newsph_nli',\n",
       " 'huggingface:newspop',\n",
       " 'huggingface:newsqa',\n",
       " 'huggingface:newsroom',\n",
       " 'huggingface:nkjp-ner',\n",
       " 'huggingface:nli_tr',\n",
       " 'huggingface:nlu_evaluation_data',\n",
       " 'huggingface:norec',\n",
       " 'huggingface:norne',\n",
       " 'huggingface:norwegian_ner',\n",
       " 'huggingface:nq_open',\n",
       " 'huggingface:nsmc',\n",
       " 'huggingface:numer_sense',\n",
       " 'huggingface:numeric_fused_head',\n",
       " 'huggingface:oclar',\n",
       " 'huggingface:offcombr',\n",
       " 'huggingface:offenseval2020_tr',\n",
       " 'huggingface:offenseval_dravidian',\n",
       " 'huggingface:ofis_publik',\n",
       " 'huggingface:ohsumed',\n",
       " 'huggingface:ollie',\n",
       " 'huggingface:omp',\n",
       " 'huggingface:onestop_english',\n",
       " 'huggingface:onestop_qa',\n",
       " 'huggingface:open_subtitles',\n",
       " 'huggingface:openai_humaneval',\n",
       " 'huggingface:openbookqa',\n",
       " 'huggingface:openslr',\n",
       " 'huggingface:openwebtext',\n",
       " 'huggingface:opinosis',\n",
       " 'huggingface:opus100',\n",
       " 'huggingface:opus_books',\n",
       " 'huggingface:opus_dgt',\n",
       " 'huggingface:opus_dogc',\n",
       " 'huggingface:opus_elhuyar',\n",
       " 'huggingface:opus_euconst',\n",
       " 'huggingface:opus_finlex',\n",
       " 'huggingface:opus_fiskmo',\n",
       " 'huggingface:opus_gnome',\n",
       " 'huggingface:opus_infopankki',\n",
       " 'huggingface:opus_memat',\n",
       " 'huggingface:opus_montenegrinsubs',\n",
       " 'huggingface:opus_openoffice',\n",
       " 'huggingface:opus_paracrawl',\n",
       " 'huggingface:opus_rf',\n",
       " 'huggingface:opus_tedtalks',\n",
       " 'huggingface:opus_ubuntu',\n",
       " 'huggingface:opus_wikipedia',\n",
       " 'huggingface:opus_xhosanavy',\n",
       " 'huggingface:orange_sum',\n",
       " 'huggingface:oscar',\n",
       " 'huggingface:para_crawl',\n",
       " 'huggingface:para_pat',\n",
       " 'huggingface:parsinlu_reading_comprehension',\n",
       " 'huggingface:pass',\n",
       " 'huggingface:paws',\n",
       " 'huggingface:paws-x',\n",
       " 'huggingface:pec',\n",
       " 'huggingface:peer_read',\n",
       " 'huggingface:peoples_daily_ner',\n",
       " 'huggingface:per_sent',\n",
       " 'huggingface:persian_ner',\n",
       " 'huggingface:pg19',\n",
       " 'huggingface:php',\n",
       " 'huggingface:piaf',\n",
       " 'huggingface:pib',\n",
       " 'huggingface:piqa',\n",
       " 'huggingface:pn_summary',\n",
       " 'huggingface:poem_sentiment',\n",
       " 'huggingface:polemo2',\n",
       " 'huggingface:poleval2019_cyberbullying',\n",
       " 'huggingface:poleval2019_mt',\n",
       " 'huggingface:polsum',\n",
       " 'huggingface:polyglot_ner',\n",
       " 'huggingface:prachathai67k',\n",
       " 'huggingface:pragmeval',\n",
       " 'huggingface:proto_qa',\n",
       " 'huggingface:psc',\n",
       " 'huggingface:ptb_text_only',\n",
       " 'huggingface:pubmed',\n",
       " 'huggingface:pubmed_qa',\n",
       " 'huggingface:py_ast',\n",
       " 'huggingface:qa4mre',\n",
       " 'huggingface:qa_srl',\n",
       " 'huggingface:qa_zre',\n",
       " 'huggingface:qangaroo',\n",
       " 'huggingface:qanta',\n",
       " 'huggingface:qasc',\n",
       " 'huggingface:qasper',\n",
       " 'huggingface:qed',\n",
       " 'huggingface:qed_amara',\n",
       " 'huggingface:quac',\n",
       " 'huggingface:quail',\n",
       " 'huggingface:quarel',\n",
       " 'huggingface:quartz',\n",
       " 'huggingface:quickdraw',\n",
       " 'huggingface:quora',\n",
       " 'huggingface:quoref',\n",
       " 'huggingface:race',\n",
       " 'huggingface:re_dial',\n",
       " 'huggingface:reasoning_bg',\n",
       " 'huggingface:recipe_nlg',\n",
       " 'huggingface:reclor',\n",
       " 'huggingface:red_caps',\n",
       " 'huggingface:reddit',\n",
       " 'huggingface:reddit_tifu',\n",
       " 'huggingface:refresd',\n",
       " 'huggingface:reuters21578',\n",
       " 'huggingface:riddle_sense',\n",
       " 'huggingface:ro_sent',\n",
       " 'huggingface:ro_sts',\n",
       " 'huggingface:ro_sts_parallel',\n",
       " 'huggingface:roman_urdu',\n",
       " 'huggingface:roman_urdu_hate_speech',\n",
       " 'huggingface:ronec',\n",
       " 'huggingface:ropes',\n",
       " 'huggingface:rotten_tomatoes',\n",
       " 'huggingface:russian_super_glue',\n",
       " 'huggingface:rvl_cdip',\n",
       " 'huggingface:s2orc',\n",
       " 'huggingface:samsum',\n",
       " 'huggingface:sanskrit_classic',\n",
       " 'huggingface:saudinewsnet',\n",
       " 'huggingface:sberquad',\n",
       " 'huggingface:sbu_captions',\n",
       " 'huggingface:scan',\n",
       " 'huggingface:scb_mt_enth_2020',\n",
       " 'huggingface:scene_parse_150',\n",
       " 'huggingface:schema_guided_dstc8',\n",
       " 'huggingface:scicite',\n",
       " 'huggingface:scielo',\n",
       " 'huggingface:scientific_papers',\n",
       " 'huggingface:scifact',\n",
       " 'huggingface:sciq',\n",
       " 'huggingface:scitail',\n",
       " 'huggingface:scitldr',\n",
       " 'huggingface:search_qa',\n",
       " 'huggingface:sede',\n",
       " 'huggingface:selqa',\n",
       " 'huggingface:sem_eval_2010_task_8',\n",
       " 'huggingface:sem_eval_2014_task_1',\n",
       " 'huggingface:sem_eval_2018_task_1',\n",
       " 'huggingface:sem_eval_2020_task_11',\n",
       " 'huggingface:sent_comp',\n",
       " 'huggingface:senti_lex',\n",
       " 'huggingface:senti_ws',\n",
       " 'huggingface:sentiment140',\n",
       " 'huggingface:sepedi_ner',\n",
       " 'huggingface:sesotho_ner_corpus',\n",
       " 'huggingface:setimes',\n",
       " 'huggingface:setswana_ner_corpus',\n",
       " 'huggingface:sharc',\n",
       " 'huggingface:sharc_modified',\n",
       " 'huggingface:sick',\n",
       " 'huggingface:silicone',\n",
       " 'huggingface:simple_questions_v2',\n",
       " 'huggingface:siswati_ner_corpus',\n",
       " 'huggingface:smartdata',\n",
       " 'huggingface:sms_spam',\n",
       " 'huggingface:snips_built_in_intents',\n",
       " 'huggingface:snli',\n",
       " 'huggingface:snow_simplified_japanese_corpus',\n",
       " 'huggingface:so_stacksample',\n",
       " 'huggingface:social_bias_frames',\n",
       " 'huggingface:social_i_qa',\n",
       " 'huggingface:sofc_materials_articles',\n",
       " 'huggingface:sogou_news',\n",
       " 'huggingface:spanish_billion_words',\n",
       " 'huggingface:spc',\n",
       " 'huggingface:species_800',\n",
       " 'huggingface:speech_commands',\n",
       " 'huggingface:spider',\n",
       " 'huggingface:squad',\n",
       " 'huggingface:squad_adversarial',\n",
       " 'huggingface:squad_es',\n",
       " 'huggingface:squad_it',\n",
       " 'huggingface:squad_kor_v1',\n",
       " 'huggingface:squad_kor_v2',\n",
       " 'huggingface:squad_v1_pt',\n",
       " 'huggingface:squad_v2',\n",
       " 'huggingface:squadshifts',\n",
       " 'huggingface:srwac',\n",
       " 'huggingface:sst',\n",
       " 'huggingface:stereoset',\n",
       " 'huggingface:story_cloze',\n",
       " 'huggingface:stsb_mt_sv',\n",
       " 'huggingface:stsb_multi_mt',\n",
       " 'huggingface:style_change_detection',\n",
       " 'huggingface:subjqa',\n",
       " 'huggingface:super_glue',\n",
       " 'huggingface:superb',\n",
       " 'huggingface:svhn',\n",
       " 'huggingface:swag',\n",
       " 'huggingface:swahili',\n",
       " 'huggingface:swahili_news',\n",
       " 'huggingface:swda',\n",
       " 'huggingface:swedish_medical_ner',\n",
       " 'huggingface:swedish_ner_corpus',\n",
       " 'huggingface:swedish_reviews',\n",
       " 'huggingface:swiss_judgment_prediction',\n",
       " 'huggingface:tab_fact',\n",
       " 'huggingface:tamilmixsentiment',\n",
       " 'huggingface:tanzil',\n",
       " 'huggingface:tapaco',\n",
       " 'huggingface:tashkeela',\n",
       " 'huggingface:taskmaster1',\n",
       " 'huggingface:taskmaster2',\n",
       " 'huggingface:taskmaster3',\n",
       " 'huggingface:tatoeba',\n",
       " 'huggingface:ted_hrlr',\n",
       " 'huggingface:ted_iwlst2013',\n",
       " 'huggingface:ted_multi',\n",
       " 'huggingface:ted_talks_iwslt',\n",
       " 'huggingface:telugu_books',\n",
       " 'huggingface:telugu_news',\n",
       " 'huggingface:tep_en_fa_para',\n",
       " 'huggingface:text2log',\n",
       " 'huggingface:textvqa',\n",
       " 'huggingface:thai_toxicity_tweet',\n",
       " 'huggingface:thainer',\n",
       " 'huggingface:thaiqa_squad',\n",
       " 'huggingface:thaisum',\n",
       " 'huggingface:the_pile',\n",
       " 'huggingface:the_pile_books3',\n",
       " 'huggingface:the_pile_openwebtext2',\n",
       " 'huggingface:the_pile_stack_exchange',\n",
       " 'huggingface:tilde_model',\n",
       " 'huggingface:time_dial',\n",
       " 'huggingface:times_of_india_news_headlines',\n",
       " 'huggingface:timit_asr',\n",
       " 'huggingface:tiny_shakespeare',\n",
       " 'huggingface:tlc',\n",
       " 'huggingface:tmu_gfm_dataset',\n",
       " 'huggingface:tne',\n",
       " 'huggingface:told-br',\n",
       " 'huggingface:totto',\n",
       " 'huggingface:trec',\n",
       " 'huggingface:trivia_qa',\n",
       " 'huggingface:truthful_qa',\n",
       " 'huggingface:tsac',\n",
       " 'huggingface:ttc4900',\n",
       " 'huggingface:tunizi',\n",
       " 'huggingface:tuple_ie',\n",
       " 'huggingface:turk',\n",
       " 'huggingface:turkic_xwmt',\n",
       " 'huggingface:turkish_movie_sentiment',\n",
       " 'huggingface:turkish_ner',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "# See all registered datasets\n",
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Cifar10 dataset and view information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to ~/tensorflow_datasets/cifar10/3.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92de72a4c65f4fe682088374be46c134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927c7805898e48ed9c484a5091d96307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6494164727bd4e19a2c445e08aea8b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incompleteQD0ELX/cifar10-train.tfrecord*...:   0%|          | 0/"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incompleteQD0ELX/cifar10-test.tfrecord*...:   0%|          | 0/1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cifar10 downloaded and prepared to ~/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\u001b[0m\n",
      "tfds.core.DatasetInfo(\n",
      "    name='cifar10',\n",
      "    full_name='cifar10/3.0.2',\n",
      "    description=\"\"\"\n",
      "    The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
      "    \"\"\",\n",
      "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
      "    data_path='~/tensorflow_datasets/cifar10/3.0.2',\n",
      "    file_format=tfrecord,\n",
      "    download_size=162.17 MiB,\n",
      "    dataset_size=132.40 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session() # Making sure we are clearing out the TensorFlow graph\n",
    "\n",
    "# Load a given dataset by name, along with the DatasetInfo\n",
    "data, info = tfds.load(\"cifar10\", with_info=True)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data \n",
    "\n",
    "Here we will print the `data` and see what it provides. Then we will need to batch the data as data is provided as individual samples when you import it from `tensorflow-datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Split('train'): <PrefetchDataset element_spec={'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'image': TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>, Split('test'): <PrefetchDataset element_spec={'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'image': TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': <tf.Tensor: shape=(16,), dtype=string, numpy=\n",
      "array([b'train_16399', b'train_01680', b'train_47917', b'train_17307',\n",
      "       b'train_27051', b'train_48736', b'train_26263', b'train_01456',\n",
      "       b'train_19135', b'train_31598', b'train_12970', b'train_04223',\n",
      "       b'train_27152', b'train_49635', b'train_04093', b'train_17537'],\n",
      "      dtype=object)>, 'image': <tf.Tensor: shape=(16, 32, 32, 3), dtype=uint8, numpy=\n",
      "array([[[[143,  96,  70],\n",
      "         [141,  96,  72],\n",
      "         [135,  93,  72],\n",
      "         ...,\n",
      "         [ 96,  37,  19],\n",
      "         [105,  42,  18],\n",
      "         [104,  38,  20]],\n",
      "\n",
      "        [[128,  98,  92],\n",
      "         [146, 118, 112],\n",
      "         [170, 145, 138],\n",
      "         ...,\n",
      "         [108,  45,  26],\n",
      "         [112,  44,  24],\n",
      "         [112,  41,  22]],\n",
      "\n",
      "        [[ 93,  69,  75],\n",
      "         [118,  96, 101],\n",
      "         [179, 160, 162],\n",
      "         ...,\n",
      "         [128,  68,  47],\n",
      "         [125,  61,  42],\n",
      "         [122,  59,  39]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[187, 150, 123],\n",
      "         [184, 148, 123],\n",
      "         [179, 142, 121],\n",
      "         ...,\n",
      "         [198, 163, 132],\n",
      "         [201, 166, 135],\n",
      "         [207, 174, 143]],\n",
      "\n",
      "        [[187, 150, 117],\n",
      "         [181, 143, 115],\n",
      "         [175, 136, 113],\n",
      "         ...,\n",
      "         [201, 164, 132],\n",
      "         [205, 168, 135],\n",
      "         [207, 171, 139]],\n",
      "\n",
      "        [[195, 161, 126],\n",
      "         [187, 153, 123],\n",
      "         [186, 151, 128],\n",
      "         ...,\n",
      "         [212, 177, 147],\n",
      "         [219, 185, 155],\n",
      "         [221, 187, 157]]],\n",
      "\n",
      "\n",
      "       [[[203, 214, 234],\n",
      "         [191, 207, 226],\n",
      "         [178, 200, 224],\n",
      "         ...,\n",
      "         [127, 172, 213],\n",
      "         [126, 171, 212],\n",
      "         [124, 170, 211]],\n",
      "\n",
      "        [[205, 214, 230],\n",
      "         [186, 199, 213],\n",
      "         [180, 197, 214],\n",
      "         ...,\n",
      "         [132, 178, 219],\n",
      "         [130, 176, 219],\n",
      "         [129, 175, 217]],\n",
      "\n",
      "        [[193, 200, 213],\n",
      "         [141, 151, 159],\n",
      "         [124, 137, 145],\n",
      "         ...,\n",
      "         [136, 178, 218],\n",
      "         [134, 177, 218],\n",
      "         [132, 176, 217]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 40,  47,  56],\n",
      "         [ 33,  37,  42],\n",
      "         [ 31,  35,  41],\n",
      "         ...,\n",
      "         [ 73,  99, 132],\n",
      "         [ 64,  91, 126],\n",
      "         [ 69,  97, 133]],\n",
      "\n",
      "        [[ 37,  44,  53],\n",
      "         [ 31,  34,  40],\n",
      "         [ 30,  34,  40],\n",
      "         ...,\n",
      "         [ 72,  98, 132],\n",
      "         [ 64,  92, 127],\n",
      "         [ 68,  96, 132]],\n",
      "\n",
      "        [[ 34,  41,  50],\n",
      "         [ 29,  32,  38],\n",
      "         [ 28,  32,  38],\n",
      "         ...,\n",
      "         [ 68,  94, 127],\n",
      "         [ 62,  89, 123],\n",
      "         [ 63,  91, 126]]],\n",
      "\n",
      "\n",
      "       [[[106, 103, 104],\n",
      "         [103,  97,  99],\n",
      "         [102,  93,  96],\n",
      "         ...,\n",
      "         [135, 126, 129],\n",
      "         [139, 130, 133],\n",
      "         [131, 122, 125]],\n",
      "\n",
      "        [[106, 104, 105],\n",
      "         [105,  99, 101],\n",
      "         [115, 106, 109],\n",
      "         ...,\n",
      "         [137, 129, 132],\n",
      "         [135, 126, 129],\n",
      "         [124, 115, 118]],\n",
      "\n",
      "        [[108, 105, 106],\n",
      "         [117, 111, 113],\n",
      "         [123, 114, 117],\n",
      "         ...,\n",
      "         [132, 123, 126],\n",
      "         [126, 117, 120],\n",
      "         [121, 112, 115]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[136, 163, 117],\n",
      "         [135, 162, 115],\n",
      "         [139, 166, 117],\n",
      "         ...,\n",
      "         [140, 144, 111],\n",
      "         [133, 135, 105],\n",
      "         [126, 125,  98]],\n",
      "\n",
      "        [[130, 150, 104],\n",
      "         [131, 150, 107],\n",
      "         [131, 150, 109],\n",
      "         ...,\n",
      "         [141, 150, 112],\n",
      "         [144, 152, 115],\n",
      "         [140, 144, 111]],\n",
      "\n",
      "        [[139, 151, 108],\n",
      "         [133, 144, 103],\n",
      "         [145, 156, 116],\n",
      "         ...,\n",
      "         [129, 137, 100],\n",
      "         [138, 144, 110],\n",
      "         [134, 136, 106]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[146, 149, 166],\n",
      "         [143, 147, 164],\n",
      "         [141, 144, 160],\n",
      "         ...,\n",
      "         [215, 194, 176],\n",
      "         [225, 204, 186],\n",
      "         [224, 203, 185]],\n",
      "\n",
      "        [[143, 149, 165],\n",
      "         [143, 148, 165],\n",
      "         [140, 144, 160],\n",
      "         ...,\n",
      "         [182, 164, 149],\n",
      "         [186, 168, 153],\n",
      "         [179, 161, 146]],\n",
      "\n",
      "        [[139, 144, 163],\n",
      "         [139, 144, 162],\n",
      "         [137, 142, 160],\n",
      "         ...,\n",
      "         [183, 161, 147],\n",
      "         [187, 165, 151],\n",
      "         [186, 164, 150]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 70,  70,  85],\n",
      "         [ 69,  69,  83],\n",
      "         [ 67,  67,  81],\n",
      "         ...,\n",
      "         [ 81,  83,  99],\n",
      "         [ 81,  84,  99],\n",
      "         [ 73,  75,  91]],\n",
      "\n",
      "        [[ 72,  71,  85],\n",
      "         [ 70,  69,  83],\n",
      "         [ 68,  67,  81],\n",
      "         ...,\n",
      "         [ 79,  81,  96],\n",
      "         [ 81,  83,  98],\n",
      "         [ 73,  75,  90]],\n",
      "\n",
      "        [[ 72,  72,  85],\n",
      "         [ 70,  70,  83],\n",
      "         [ 69,  69,  81],\n",
      "         ...,\n",
      "         [ 79,  81,  95],\n",
      "         [ 81,  83,  97],\n",
      "         [ 73,  75,  89]]],\n",
      "\n",
      "\n",
      "       [[[133, 151, 172],\n",
      "         [129, 148, 172],\n",
      "         [131, 153, 173],\n",
      "         ...,\n",
      "         [226, 221, 211],\n",
      "         [228, 224, 215],\n",
      "         [218, 213, 207]],\n",
      "\n",
      "        [[135, 152, 174],\n",
      "         [130, 148, 174],\n",
      "         [132, 152, 176],\n",
      "         ...,\n",
      "         [215, 211, 201],\n",
      "         [213, 208, 200],\n",
      "         [203, 198, 193]],\n",
      "\n",
      "        [[142, 158, 179],\n",
      "         [133, 149, 177],\n",
      "         [131, 149, 175],\n",
      "         ...,\n",
      "         [206, 201, 194],\n",
      "         [203, 199, 193],\n",
      "         [197, 192, 188]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 82,  91,  97],\n",
      "         [ 92, 105, 109],\n",
      "         [ 92, 108, 118],\n",
      "         ...,\n",
      "         [ 86, 105, 113],\n",
      "         [ 92, 109, 116],\n",
      "         [ 90, 105, 114]],\n",
      "\n",
      "        [[ 88,  93,  99],\n",
      "         [ 96, 106, 110],\n",
      "         [ 94, 108, 117],\n",
      "         ...,\n",
      "         [ 87, 103, 117],\n",
      "         [ 91, 105, 115],\n",
      "         [ 91, 103, 116]],\n",
      "\n",
      "        [[ 88,  90,  97],\n",
      "         [ 94, 100, 105],\n",
      "         [ 91,  99, 108],\n",
      "         ...,\n",
      "         [ 89, 103, 117],\n",
      "         [ 87, 100, 111],\n",
      "         [ 90, 102, 114]]],\n",
      "\n",
      "\n",
      "       [[[ 75,  75,  80],\n",
      "         [ 75,  77,  81],\n",
      "         [ 81,  83,  88],\n",
      "         ...,\n",
      "         [ 71,  72,  70],\n",
      "         [ 68,  70,  68],\n",
      "         [ 68,  69,  64]],\n",
      "\n",
      "        [[ 72,  75,  80],\n",
      "         [ 77,  80,  87],\n",
      "         [ 82,  83,  87],\n",
      "         ...,\n",
      "         [ 71,  71,  70],\n",
      "         [ 67,  69,  68],\n",
      "         [ 67,  68,  63]],\n",
      "\n",
      "        [[ 73,  72,  75],\n",
      "         [ 79,  78,  83],\n",
      "         [ 91,  82,  79],\n",
      "         ...,\n",
      "         [ 67,  69,  66],\n",
      "         [ 66,  68,  68],\n",
      "         [ 67,  67,  64]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[118,  80,  46],\n",
      "         [114,  78,  46],\n",
      "         [112,  77,  45],\n",
      "         ...,\n",
      "         [109,  74,  40],\n",
      "         [107,  72,  38],\n",
      "         [106,  71,  37]],\n",
      "\n",
      "        [[122,  82,  45],\n",
      "         [118,  81,  46],\n",
      "         [121,  84,  48],\n",
      "         ...,\n",
      "         [114,  77,  42],\n",
      "         [113,  76,  40],\n",
      "         [115,  78,  41]],\n",
      "\n",
      "        [[123,  81,  45],\n",
      "         [119,  81,  47],\n",
      "         [120,  82,  46],\n",
      "         ...,\n",
      "         [128,  93,  60],\n",
      "         [129,  94,  61],\n",
      "         [123,  91,  58]]]], dtype=uint8)>, 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([7, 8, 4, 4, 6, 5, 2, 9, 6, 6, 9, 9, 3, 0, 8, 7])>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 18:45:21.326934: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Print some training data\n",
    "train_ds = data[\"train\"].batch(16)\n",
    "for item in train_ds:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(16, 32, 32, 3), dtype=uint8, numpy=\n",
      "array([[[[143,  96,  70],\n",
      "         [141,  96,  72],\n",
      "         [135,  93,  72],\n",
      "         ...,\n",
      "         [ 96,  37,  19],\n",
      "         [105,  42,  18],\n",
      "         [104,  38,  20]],\n",
      "\n",
      "        [[128,  98,  92],\n",
      "         [146, 118, 112],\n",
      "         [170, 145, 138],\n",
      "         ...,\n",
      "         [108,  45,  26],\n",
      "         [112,  44,  24],\n",
      "         [112,  41,  22]],\n",
      "\n",
      "        [[ 93,  69,  75],\n",
      "         [118,  96, 101],\n",
      "         [179, 160, 162],\n",
      "         ...,\n",
      "         [128,  68,  47],\n",
      "         [125,  61,  42],\n",
      "         [122,  59,  39]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[187, 150, 123],\n",
      "         [184, 148, 123],\n",
      "         [179, 142, 121],\n",
      "         ...,\n",
      "         [198, 163, 132],\n",
      "         [201, 166, 135],\n",
      "         [207, 174, 143]],\n",
      "\n",
      "        [[187, 150, 117],\n",
      "         [181, 143, 115],\n",
      "         [175, 136, 113],\n",
      "         ...,\n",
      "         [201, 164, 132],\n",
      "         [205, 168, 135],\n",
      "         [207, 171, 139]],\n",
      "\n",
      "        [[195, 161, 126],\n",
      "         [187, 153, 123],\n",
      "         [186, 151, 128],\n",
      "         ...,\n",
      "         [212, 177, 147],\n",
      "         [219, 185, 155],\n",
      "         [221, 187, 157]]],\n",
      "\n",
      "\n",
      "       [[[203, 214, 234],\n",
      "         [191, 207, 226],\n",
      "         [178, 200, 224],\n",
      "         ...,\n",
      "         [127, 172, 213],\n",
      "         [126, 171, 212],\n",
      "         [124, 170, 211]],\n",
      "\n",
      "        [[205, 214, 230],\n",
      "         [186, 199, 213],\n",
      "         [180, 197, 214],\n",
      "         ...,\n",
      "         [132, 178, 219],\n",
      "         [130, 176, 219],\n",
      "         [129, 175, 217]],\n",
      "\n",
      "        [[193, 200, 213],\n",
      "         [141, 151, 159],\n",
      "         [124, 137, 145],\n",
      "         ...,\n",
      "         [136, 178, 218],\n",
      "         [134, 177, 218],\n",
      "         [132, 176, 217]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 40,  47,  56],\n",
      "         [ 33,  37,  42],\n",
      "         [ 31,  35,  41],\n",
      "         ...,\n",
      "         [ 73,  99, 132],\n",
      "         [ 64,  91, 126],\n",
      "         [ 69,  97, 133]],\n",
      "\n",
      "        [[ 37,  44,  53],\n",
      "         [ 31,  34,  40],\n",
      "         [ 30,  34,  40],\n",
      "         ...,\n",
      "         [ 72,  98, 132],\n",
      "         [ 64,  92, 127],\n",
      "         [ 68,  96, 132]],\n",
      "\n",
      "        [[ 34,  41,  50],\n",
      "         [ 29,  32,  38],\n",
      "         [ 28,  32,  38],\n",
      "         ...,\n",
      "         [ 68,  94, 127],\n",
      "         [ 62,  89, 123],\n",
      "         [ 63,  91, 126]]],\n",
      "\n",
      "\n",
      "       [[[106, 103, 104],\n",
      "         [103,  97,  99],\n",
      "         [102,  93,  96],\n",
      "         ...,\n",
      "         [135, 126, 129],\n",
      "         [139, 130, 133],\n",
      "         [131, 122, 125]],\n",
      "\n",
      "        [[106, 104, 105],\n",
      "         [105,  99, 101],\n",
      "         [115, 106, 109],\n",
      "         ...,\n",
      "         [137, 129, 132],\n",
      "         [135, 126, 129],\n",
      "         [124, 115, 118]],\n",
      "\n",
      "        [[108, 105, 106],\n",
      "         [117, 111, 113],\n",
      "         [123, 114, 117],\n",
      "         ...,\n",
      "         [132, 123, 126],\n",
      "         [126, 117, 120],\n",
      "         [121, 112, 115]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[136, 163, 117],\n",
      "         [135, 162, 115],\n",
      "         [139, 166, 117],\n",
      "         ...,\n",
      "         [140, 144, 111],\n",
      "         [133, 135, 105],\n",
      "         [126, 125,  98]],\n",
      "\n",
      "        [[130, 150, 104],\n",
      "         [131, 150, 107],\n",
      "         [131, 150, 109],\n",
      "         ...,\n",
      "         [141, 150, 112],\n",
      "         [144, 152, 115],\n",
      "         [140, 144, 111]],\n",
      "\n",
      "        [[139, 151, 108],\n",
      "         [133, 144, 103],\n",
      "         [145, 156, 116],\n",
      "         ...,\n",
      "         [129, 137, 100],\n",
      "         [138, 144, 110],\n",
      "         [134, 136, 106]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[146, 149, 166],\n",
      "         [143, 147, 164],\n",
      "         [141, 144, 160],\n",
      "         ...,\n",
      "         [215, 194, 176],\n",
      "         [225, 204, 186],\n",
      "         [224, 203, 185]],\n",
      "\n",
      "        [[143, 149, 165],\n",
      "         [143, 148, 165],\n",
      "         [140, 144, 160],\n",
      "         ...,\n",
      "         [182, 164, 149],\n",
      "         [186, 168, 153],\n",
      "         [179, 161, 146]],\n",
      "\n",
      "        [[139, 144, 163],\n",
      "         [139, 144, 162],\n",
      "         [137, 142, 160],\n",
      "         ...,\n",
      "         [183, 161, 147],\n",
      "         [187, 165, 151],\n",
      "         [186, 164, 150]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 70,  70,  85],\n",
      "         [ 69,  69,  83],\n",
      "         [ 67,  67,  81],\n",
      "         ...,\n",
      "         [ 81,  83,  99],\n",
      "         [ 81,  84,  99],\n",
      "         [ 73,  75,  91]],\n",
      "\n",
      "        [[ 72,  71,  85],\n",
      "         [ 70,  69,  83],\n",
      "         [ 68,  67,  81],\n",
      "         ...,\n",
      "         [ 79,  81,  96],\n",
      "         [ 81,  83,  98],\n",
      "         [ 73,  75,  90]],\n",
      "\n",
      "        [[ 72,  72,  85],\n",
      "         [ 70,  70,  83],\n",
      "         [ 69,  69,  81],\n",
      "         ...,\n",
      "         [ 79,  81,  95],\n",
      "         [ 81,  83,  97],\n",
      "         [ 73,  75,  89]]],\n",
      "\n",
      "\n",
      "       [[[133, 151, 172],\n",
      "         [129, 148, 172],\n",
      "         [131, 153, 173],\n",
      "         ...,\n",
      "         [226, 221, 211],\n",
      "         [228, 224, 215],\n",
      "         [218, 213, 207]],\n",
      "\n",
      "        [[135, 152, 174],\n",
      "         [130, 148, 174],\n",
      "         [132, 152, 176],\n",
      "         ...,\n",
      "         [215, 211, 201],\n",
      "         [213, 208, 200],\n",
      "         [203, 198, 193]],\n",
      "\n",
      "        [[142, 158, 179],\n",
      "         [133, 149, 177],\n",
      "         [131, 149, 175],\n",
      "         ...,\n",
      "         [206, 201, 194],\n",
      "         [203, 199, 193],\n",
      "         [197, 192, 188]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 82,  91,  97],\n",
      "         [ 92, 105, 109],\n",
      "         [ 92, 108, 118],\n",
      "         ...,\n",
      "         [ 86, 105, 113],\n",
      "         [ 92, 109, 116],\n",
      "         [ 90, 105, 114]],\n",
      "\n",
      "        [[ 88,  93,  99],\n",
      "         [ 96, 106, 110],\n",
      "         [ 94, 108, 117],\n",
      "         ...,\n",
      "         [ 87, 103, 117],\n",
      "         [ 91, 105, 115],\n",
      "         [ 91, 103, 116]],\n",
      "\n",
      "        [[ 88,  90,  97],\n",
      "         [ 94, 100, 105],\n",
      "         [ 91,  99, 108],\n",
      "         ...,\n",
      "         [ 89, 103, 117],\n",
      "         [ 87, 100, 111],\n",
      "         [ 90, 102, 114]]],\n",
      "\n",
      "\n",
      "       [[[ 75,  75,  80],\n",
      "         [ 75,  77,  81],\n",
      "         [ 81,  83,  88],\n",
      "         ...,\n",
      "         [ 71,  72,  70],\n",
      "         [ 68,  70,  68],\n",
      "         [ 68,  69,  64]],\n",
      "\n",
      "        [[ 72,  75,  80],\n",
      "         [ 77,  80,  87],\n",
      "         [ 82,  83,  87],\n",
      "         ...,\n",
      "         [ 71,  71,  70],\n",
      "         [ 67,  69,  68],\n",
      "         [ 67,  68,  63]],\n",
      "\n",
      "        [[ 73,  72,  75],\n",
      "         [ 79,  78,  83],\n",
      "         [ 91,  82,  79],\n",
      "         ...,\n",
      "         [ 67,  69,  66],\n",
      "         [ 66,  68,  68],\n",
      "         [ 67,  67,  64]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[118,  80,  46],\n",
      "         [114,  78,  46],\n",
      "         [112,  77,  45],\n",
      "         ...,\n",
      "         [109,  74,  40],\n",
      "         [107,  72,  38],\n",
      "         [106,  71,  37]],\n",
      "\n",
      "        [[122,  82,  45],\n",
      "         [118,  81,  46],\n",
      "         [121,  84,  48],\n",
      "         ...,\n",
      "         [114,  77,  42],\n",
      "         [113,  76,  40],\n",
      "         [115,  78,  41]],\n",
      "\n",
      "        [[123,  81,  45],\n",
      "         [119,  81,  47],\n",
      "         [120,  82,  46],\n",
      "         ...,\n",
      "         [128,  93,  60],\n",
      "         [129,  94,  61],\n",
      "         [123,  91,  58]]]], dtype=uint8)>, <tf.Tensor: shape=(16, 10), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 18:45:21.386920: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Defining a dataset with batch size 16\n",
    "train_ds = data[\"train\"].batch(16)\n",
    "\n",
    "# Creating a dataset that returns an (image, one-hot label) tuple\n",
    "def format_data(x):\n",
    "    return (x[\"image\"], tf.one_hot(x[\"label\"], depth=10))\n",
    "train_ds = train_ds.map(format_data)\n",
    "\n",
    "# Iterating the dataset\n",
    "for item in train_ds:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a simple CNN on the Cifar10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3125/3125 [==============================] - 6s 2ms/step - loss: 4.4501 - acc: 0.1123\n",
      "Epoch 2/25\n",
      "3125/3125 [==============================] - 8s 2ms/step - loss: 2.2683 - acc: 0.1431\n",
      "Epoch 3/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 2.2360 - acc: 0.1593\n",
      "Epoch 4/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 2.2046 - acc: 0.1717\n",
      "Epoch 5/25\n",
      "3125/3125 [==============================] - 8s 2ms/step - loss: 2.1628 - acc: 0.1836\n",
      "Epoch 6/25\n",
      "3125/3125 [==============================] - 7s 2ms/step - loss: 2.1324 - acc: 0.1953\n",
      "Epoch 7/25\n",
      "3125/3125 [==============================] - 8s 2ms/step - loss: 2.1024 - acc: 0.2064\n",
      "Epoch 8/25\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 2.0863 - acc: 0.2124\n",
      "Epoch 9/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 2.0398 - acc: 0.2272\n",
      "Epoch 10/25\n",
      "3125/3125 [==============================] - 8s 2ms/step - loss: 2.0352 - acc: 0.2326\n",
      "Epoch 11/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 2.0049 - acc: 0.2418\n",
      "Epoch 12/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.9864 - acc: 0.2499\n",
      "Epoch 13/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.9728 - acc: 0.2584\n",
      "Epoch 14/25\n",
      "3125/3125 [==============================] - 7s 2ms/step - loss: 1.9614 - acc: 0.2624\n",
      "Epoch 15/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.9807 - acc: 0.2560\n",
      "Epoch 16/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.9396 - acc: 0.2681\n",
      "Epoch 17/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.9253 - acc: 0.2753\n",
      "Epoch 18/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.9144 - acc: 0.2830\n",
      "Epoch 19/25\n",
      "3125/3125 [==============================] - 8s 2ms/step - loss: 1.9296 - acc: 0.2801\n",
      "Epoch 20/25\n",
      "3125/3125 [==============================] - 7s 2ms/step - loss: 1.9040 - acc: 0.2870\n",
      "Epoch 21/25\n",
      "3125/3125 [==============================] - 8s 2ms/step - loss: 1.9055 - acc: 0.2868\n",
      "Epoch 22/25\n",
      "3125/3125 [==============================] - 8s 2ms/step - loss: 1.8949 - acc: 0.2912\n",
      "Epoch 23/25\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 1.8698 - acc: 0.2980\n",
      "Epoch 24/25\n",
      "3125/3125 [==============================] - 7s 2ms/step - loss: 1.8629 - acc: 0.2995\n",
      "Epoch 25/25\n",
      "3125/3125 [==============================] - 7s 2ms/step - loss: 1.8588 - acc: 0.3029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42bb3d4e50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Defining a simple convolution neural network to process the CIFAR data\n",
    "model = Sequential([\n",
    "    Conv2D(64,(5,5), activation='relu', input_shape=(32,32,3)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Fitting the model on the data for 25 epochs\n",
    "model.fit(train_ds, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
