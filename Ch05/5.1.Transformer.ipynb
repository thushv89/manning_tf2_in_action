{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-of-the-art in deep learning: Transformers\n",
    "\n",
    "Transformers are the latest addition to the deep learning portfolio. Transformer networks started from the natural language processing domain and has permeated to other areas like computer vision. This was introduced in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et. al. In this chapter, we are going to look at various technical details of how to implement a Transformer ourselves. The Transformer has two main parts.\n",
    "\n",
    "* Encoder \n",
    "* Decoder\n",
    "\n",
    "Both the encoder and the decoder has multiple layers in it, and each layer consists of following types of sub layers.\n",
    "\n",
    "* Self-attention layer - This layer allows to pay attention to other inputs words in the sequence while processing a given input word. For example if the model is processing the word \"it\" from the sentence \"I kicked the ball and it disappeared\", this layer can pay attention the word \"ball\" while processing the word \"it\". This ability is quite important when it comes to natural language related tasks.\n",
    "* Fully connected layer - This adds depth to the model, allowing the model to learn more and more high level features. \n",
    "\n",
    "In this notebook you will learn,\n",
    "\n",
    "* The computations used in the self-attention and fully-connected sub layers\n",
    "* How to use the sub-classing API to create custom Self-Attention and Fully Connected layers\n",
    "* Use TensorFlow opeartions to define input masks\n",
    "* Define the Encoder and Decoder as layers\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch05/5.1.Transformer.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inputs and variables\n",
    "\n",
    "Here we define an input sequence (having 7 timesteps) and the respective variables that will be in a self-attention layer. The input to the model is a batch of sentences, and each sentence is a sequence of words, and each word is represented as a vector (e.g. word embeddings). In summary, the input has following dimensions.\n",
    "\n",
    "* Batch dimension (1) - In our example, a batch will have a single sentence\n",
    "* Time dimension (7) - In our example, the sentence has 7 words\n",
    "* Feature dimension (512) - In our example, each word is represented by a 512 elements long word vector\n",
    "\n",
    "The weights are of size $n \\times n$, where $n$ is the feature dimensionality of the input to that layer. This means that during the computations, the weight matrix takes a $b \\times t \\times n$ input and prodicues a $b \\times t \\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=(1, 7, 512)\n",
      "Wq.shape=(512, 512)\n",
      "Wk.shape=(512, 512)\n",
      "Wv.shape=(512, 512)\n"
     ]
    }
   ],
   "source": [
    "# Section 5.2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n_seq = 7 # Sequence length\n",
    "\n",
    "# Defining the input\n",
    "x = tf.constant(np.random.normal(size=(1,n_seq,512)), dtype='float32')\n",
    "\n",
    "# Query, Key and Value matrices\n",
    "Wq = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "Wk = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "Wv = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "\n",
    "# Printing the shapes of the data\n",
    "print('x.shape={}'.format(x.shape))\n",
    "print('Wq.shape={}'.format(Wq.shape))\n",
    "print('Wk.shape={}'.format(Wk.shape))\n",
    "print('Wv.shape={}'.format(Wv.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the SelfAttention layer\n",
    "\n",
    "Here we are going to define the self-attention mechanism encapsulated in a Keras layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "# Section 5.2\n",
    "# Code listing 5.1\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class SelfAttentionLayer(layers.Layer):\n",
    "    \"\"\" Defines the computations in the self attention layer \"\"\"\n",
    "    def __init__(self, d):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        # Feature dimensionality of the output\n",
    "        self.d = d\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Query weight matrix\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        # Key weight matrix\n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        # Value weight matrix\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "    \n",
    "    def call(self, q_x, k_x, v_x):\n",
    "        # Computing query, key and value\n",
    "        q = tf.matmul(q_x,self.Wq)\n",
    "        k = tf.matmul(k_x,self.Wk)\n",
    "        v = tf.matmul(v_x,self.Wv)\n",
    "        \n",
    "        # Computing the probability matrix\n",
    "        p = tf.nn.softmax(tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d))\n",
    "        p = tf.squeeze(p)\n",
    "\n",
    "        # Computing the final output\n",
    "        h = tf.matmul(p, v)\n",
    "        return h,p\n",
    "\n",
    "# Creating a dummy self attention layer\n",
    "layer = SelfAttentionLayer(512)\n",
    "# Getting the output\n",
    "h, p = layer(x, x, x)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Self-Attention in a Multi-head Attention Layer\n",
    "\n",
    "In the Transformer network found in the original paper, the self-attention layers are found as multi-head attention layers. This means each layer has multiple smaller attention layers. This enables the model to learn more robust features than when having a single big attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "# Defining an array of SelfAttentionLayer objects forming a multi-head attention layer\n",
    "multi_attn_head = [SelfAttentionLayer(64) for i in range(8)]\n",
    "outputs = [head(x, x, x)[0] for head in multi_attn_head]\n",
    "\n",
    "# Concatting the individual outputs to a single output\n",
    "outputs = tf.concat(outputs, axis=-1)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a SelfAttention layer with Masking Capability\n",
    "\n",
    "The decoder model uses a special type of attention layer. This attention layer masks any words ahead of the current word being processed. This in turn helps to avoid the decoder from seeing words it shouldn't during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "# Section 5.2\n",
    "# Code listing 5.2\n",
    "import tensorflow as tf\n",
    "\n",
    "class SelfAttentionLayer(layers.Layer):\n",
    "    \"\"\" Defines the computations in the self attention layer \"\"\"\n",
    "    \n",
    "    def __init__(self, d):        \n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        # Feature dimensionality of the output\n",
    "        self.d = d\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Query weight matrix\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        # Key weight matrix\n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        # Value weight matrix\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "    \n",
    "    def call(self, q_x, k_x, v_x, mask=None):\n",
    "        # Computing query, key and value\n",
    "        q = tf.matmul(q_x,self.Wq) #[None, t, d]\n",
    "        k = tf.matmul(k_x,self.Wk) #[None, t, d]\n",
    "        v = tf.matmul(v_x,self.Wv) #[None, t, d]\n",
    "        \n",
    "        # Computing the probability matrix\n",
    "        p = tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d) # [None, t, t]\n",
    "                \n",
    "        if mask is None:\n",
    "            p = tf.nn.softmax(p)\n",
    "        else:\n",
    "            # Creating the mask\n",
    "            p += mask * -1e9\n",
    "            p = tf.nn.softmax(p)\n",
    "                \n",
    "        # Computing the final output\n",
    "        h = tf.matmul(p, v) # [None, t, t] . [None, t, d] => [None, t, d]\n",
    "        return h,p\n",
    "\n",
    "layer = SelfAttentionLayer(512)\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n",
    "h, p = layer(x, x, x, mask)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Mask using TensorFlow\n",
    "\n",
    "Let's now define an example mask in TensorFlow. As we have defined an input sequence having 7 steps, let's define a mask for 7 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]], shape=(7, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Creating a dummy mask\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "multi_attn_head = [SelfAttentionLayer(64) for i in range(8)]\n",
    "outputs = [head(x, x, x)[0] for head in multi_attn_head]\n",
    "outputs = tf.concat(outputs, axis=-1)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Fully-Connected Layer\n",
    "\n",
    "Now let's define the fully-connected sublayer found in the Transformer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "# Section 5.2\n",
    "# Code listing 5.3\n",
    "import tensorflow as tf\n",
    "\n",
    "class FCLayer(layers.Layer):\n",
    "    \"\"\" The computations of a fully connected sublayer \"\"\"\n",
    "    def __init__(self, d1, d2):\n",
    "        super(FCLayer, self).__init__()\n",
    "        # Dimensionality of the first hidden layer\n",
    "        self.d1 = d1\n",
    "        # Dimensionality of the second hidden layer\n",
    "        self.d2 = d2\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # First layer's weights and biases\n",
    "        self.W1 = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d1), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.b1 = self.add_weight(\n",
    "            shape=(self.d1,), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        # Second layer's weights and biases\n",
    "        self.W2 = self.add_weight(\n",
    "            shape=(self.d1, self.d2), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.b2 = self.add_weight(\n",
    "            shape=(self.d2,), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )  \n",
    "    \n",
    "    def call(self, x):\n",
    "        # Computing the first fully connected output\n",
    "        ff1 = tf.nn.relu(tf.matmul(x,self.W1)+self.b1)\n",
    "        # Computing the second fully connected output\n",
    "        # Note that the second layer doesn't use an activation\n",
    "        ff2 = tf.matmul(ff1, self.W2)+self.b2\n",
    "        return ff2\n",
    "    \n",
    "# Creating a dummy fully connected layer\n",
    "ff = FCLayer(2048, 512)(h)\n",
    "print(ff.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Encoder Layer and the Decoder Layer\n",
    "\n",
    "With all the low-level details ironed out, let's define the encoder and decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5.2\n",
    "# Code listing 5.4\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class EncoderLayer(layers.Layer):\n",
    "    \"\"\" The Encoder layer \"\"\"\n",
    "    \n",
    "    def __init__(self, d, n_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Feature dimensionality\n",
    "        self.d = d\n",
    "        # Dimensionality of a head\n",
    "        self.d_head = int(d/n_heads) \n",
    "        # Number of heads\n",
    "        self.n_heads = n_heads\n",
    "        # Actual attention heads\n",
    "        self.attn_heads = [SelfAttentionLayer(self.d_head) for i in range(self.n_heads)]\n",
    "        # Fully connected layer\n",
    "        self.fc_layer = FCLayer(2048, self.d)\n",
    " \n",
    "    def call(self, x):\n",
    "        \n",
    "        def compute_multihead_output(x):\n",
    "            \"\"\" Computing the multi head attention output\"\"\"\n",
    "            outputs = [head(x, x, x)[0] for head in self.attn_heads]            \n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "            return outputs\n",
    "        \n",
    "        # Multi head attention layer output\n",
    "        h1 = compute_multihead_output(x)\n",
    "        # Fully connected layer output\n",
    "        y = self.fc_layer(h1)\n",
    "        \n",
    "        return y\n",
    "\n",
    "# Code listing 5.5\n",
    "class DecoderLayer(layers.Layer):\n",
    "    \"\"\" The decoder layer \"\"\"\n",
    "    def __init__(self, d, n_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Feature dimensionality\n",
    "        self.d = d\n",
    "        # Dimensionality of a single head\n",
    "        self.d_head = int(d/n_heads)\n",
    "        # Actual self attention heads (decoder inputs)\n",
    "        self.dec_attn_heads = [SelfAttentionLayer(self.d_head) for i in range(n_heads)]\n",
    "        # Actual self attention heads (encoder outputs)\n",
    "        self.attn_heads = [SelfAttentionLayer(self.d_head) for i in range(n_heads)]\n",
    "        # Fully connected layer\n",
    "        self.fc_layer = FCLayer(2048, self.d)\n",
    "        \n",
    "    def call(self, de_x, en_x, mask=None):\n",
    "        \n",
    "        def compute_multihead_output(attn_heads, de_x, en_x, mask=None):\n",
    "            \"\"\" Computing the multi head attention output\"\"\"\n",
    "            outputs = [head(en_x, en_x, de_x, mask)[0] for head in attn_heads]\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "            return outputs\n",
    "        \n",
    "        # Multi head attention layer output (from decoder inputs)\n",
    "        h1 = compute_multihead_output(self.dec_attn_heads, de_x, de_x, mask)        \n",
    "        # Multi head attention layer output (from encoder outputs)\n",
    "        h2 = compute_multihead_output(self.attn_heads, h1, en_x)\n",
    "        y = self.fc_layer(h2)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Simple Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MinTransformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 25, 512)      153600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer (EncoderLayer)    (None, 25, 512)      2886144     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 25, 512)      204800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer_1 (EncoderLayer)  (None, 25, 512)      2886144     encoder_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer (DecoderLayer)    (None, 25, 512)      3672576     embedding_1[0][0]                \n",
      "                                                                 encoder_layer_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer_1 (DecoderLayer)  (None, 25, 512)      3672576     decoder_layer[0][0]              \n",
      "                                                                 encoder_layer_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 25, 400)      205200      decoder_layer_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 13,681,040\n",
      "Trainable params: 13,681,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 5.2\n",
    "# Code listing 5.6\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Defining some hyperparameters\n",
    "n_steps = 25 # Sequence length\n",
    "n_en_vocab = 300 # Encoder's vocabulary size\n",
    "n_de_vocab = 400 # Decoder's vocabulary size\n",
    "n_heads = 8 # Number of attention heads\n",
    "d = 512 # The feature dimensionality of each layer\n",
    "\n",
    "# Look-ahead mask\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((n_steps, n_steps)), -1, 0)\n",
    "\n",
    "# Encoder input layer\n",
    "en_inp = layers.Input(shape=(n_steps,))\n",
    "# Encoder input embedddings\n",
    "en_emb = layers.Embedding(n_en_vocab, 512, input_length=n_steps)(en_inp)\n",
    "\n",
    "# Two encoder layers\n",
    "en_out1 = EncoderLayer(d, n_heads)(en_emb)\n",
    "en_out2 = EncoderLayer(d, n_heads)(en_out1)\n",
    "\n",
    "# Decoder input layer\n",
    "de_inp = layers.Input(shape=(n_steps,))\n",
    "# Decoder input embeddings\n",
    "de_emb = layers.Embedding(n_de_vocab, 512, input_length=n_steps)(de_inp)\n",
    "\n",
    "# Two decoder layers\n",
    "de_out1 = DecoderLayer(d, n_heads)(de_emb, en_out2, mask)\n",
    "de_out2 = DecoderLayer(d, n_heads)(de_out1, en_out2, mask)\n",
    "\n",
    "# Final output layer\n",
    "de_pred = layers.Dense(n_de_vocab, activation='softmax')(de_out2)\n",
    "\n",
    "# Defining the model\n",
    "transformer = models.Model(inputs=[en_inp, de_inp], outputs=de_pred, name='MinTransformer')\n",
    "# Compiling the model\n",
    "transformer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
