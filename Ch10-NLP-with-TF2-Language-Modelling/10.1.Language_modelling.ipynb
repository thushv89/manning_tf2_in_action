{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling (Natural Language Processing)\n",
    "\n",
    "Natural Language Processing (NLP) is a vast subject with many different specializations. Here we are going to discuss a topic that gave rise to ground breaking models like BERT that changed the NLP landscape dramatically; language modelling. Language modelling is an unsupervised training method, where you ask a model to predict the next character/word/sentence given the previous characters/words/sentences.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch10-NLP-with-TF2-Language-Modelling/10.1.Language_modelling.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 07:59:08.189933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 07:59:08.195356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 07:59:08.195702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import requests\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "from PIL.PngImagePlugin import PngImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the bAbI's children story dataset\n",
    "\n",
    "For this task, we'll be using a popular children story dataset from the [bAbI project](https://research.fb.com/downloads/babi/) of Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tar file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Section 10.1\n",
    "\n",
    "# Code listing 10.1\n",
    "\n",
    "# Downloading the data\n",
    "# http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data', 'lm','CBTest.tgz')):\n",
    "    url = \"http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists(os.path.join('data','lm')):\n",
    "        os.makedirs(os.path.join('data','lm'))\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data', 'lm', 'CBTest.tgz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "          \n",
    "else:\n",
    "    print(\"The tar file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'lm', 'CBTest')):\n",
    "    # Write to a file\n",
    "    tarf = tarfile.open(os.path.join(\"data\",\"lm\",\"CBTest.tgz\"))\n",
    "    tarf.extractall(os.path.join(\"data\",\"lm\"))  \n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "After downloading the data, let's read that into memory. There are three sets; training validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code listing 10.2\n",
    "\n",
    "def read_data(path):\n",
    "    stories = []\n",
    "\n",
    "    with open(path, 'r') as f:    \n",
    "        s = [] \n",
    "        for row in f:\n",
    "            \n",
    "            if row.startswith(\"_BOOK_TITLE_\"):\n",
    "                if len(s)>0:\n",
    "                    stories.append(' '.join(s).lower())            \n",
    "                s = []           \n",
    "\n",
    "            s.append(row)\n",
    "            \n",
    "    if len(s)>0:\n",
    "        stories.append(' '.join(s).lower())  \n",
    "    \n",
    "    return stories\n",
    "\n",
    "stories = read_data(os.path.join('data','lm','CBTest','data','cbt_train.txt'))\n",
    "val_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_valid.txt'))\n",
    "test_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 98 stories (train)\n",
      "Collected 5 stories (valid)\n",
      "Collected 5 stories (test)\n",
      "_book_title_ : andrew_lang___prince_prigio.txt.out\n",
      " chapter i. -lcb- chapter heading picture : p1.jp\n",
      "\n",
      " _book_title_ : andrew_lang___the_violet_fairy_book.txt.out\n",
      " a tale of the tontlawald long , long ago\n"
     ]
    }
   ],
   "source": [
    "print(\"Collected {} stories (train)\".format(len(stories)))\n",
    "print(\"Collected {} stories (valid)\".format(len(val_stories)))\n",
    "print(\"Collected {} stories (test)\".format(len(test_stories)))\n",
    "print(stories[0][:100])\n",
    "print('\\n', stories[10][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99464\n",
      "99832\n",
      "136758\n",
      "761257\n",
      "524783\n",
      "522998\n",
      "528840\n",
      "531058\n",
      "527598\n",
      "674648\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(stories[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick drive to vocabulary-ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",      348650\n",
      "the    242890\n",
      ".\\n    192549\n",
      "and    179205\n",
      "to     120821\n",
      "a      101990\n",
      "of      96748\n",
      "i       79780\n",
      "he      78129\n",
      "was     66593\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 14473\n"
     ]
    }
   ],
   "source": [
    "# Section 10.1\n",
    "\n",
    "from collections import Counter\n",
    "# Create a large list which contains all the words in all the reviews\n",
    "data_list = [w for doc in stories for w in doc.split(' ')]\n",
    "\n",
    "# Create a Counter object from that list\n",
    "# Counter returns a dictionary, where key is a word and the value is the frequency\n",
    "cnt = Counter(data_list)\n",
    "\n",
    "# Convert the result to a pd.Series \n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "\n",
    "# Print most common words\n",
    "print(freq_df.head(n=10))\n",
    "\n",
    "# Count of words >= n frequent\n",
    "n=10\n",
    "print(\"\\nVocabulary size (>={} frequent): {}\".format(n, (freq_df>=n).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert strings to n-grams\n",
    "\n",
    "For our language modelling task, we're going to split strings into bigrams. That is, given the string\n",
    "\n",
    "`i went to the office`, it is converted to,\n",
    "\n",
    "`[\"i \", \"we\", \"nt\", \" t\", \"o \", \"th\", \"e \", \"of\", \"fi\", \"ce\"]`\n",
    "\n",
    "We will also look at what are the most common bigrams and some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I like chocolates\n",
      "\t1-grams: ['I', ' ', 'l', 'i', 'k', 'e', ' ', 'c', 'h', 'o', 'c', 'o', 'l', 'a', 't', 'e', 's']\n",
      "\t2-grams: ['I ', 'li', 'ke', ' c', 'ho', 'co', 'la', 'te', 's']\n",
      "\t3-grams: ['I l', 'ike', ' ch', 'oco', 'lat', 'es']\n",
      "\n",
      "Sample of most-common bigrams\n",
      "e     455626\n",
      " t    344361\n",
      "he    310227\n",
      "d     309291\n",
      "th    284237\n",
      " a    268358\n",
      "t     257890\n",
      "s     228249\n",
      " h    192591\n",
      " s    183193\n",
      "dtype: int64\n",
      "\n",
      "Median: 143.5\n",
      "\n",
      "count      1070.000000\n",
      "mean      12152.204673\n",
      "std       36425.625033\n",
      "min           1.000000\n",
      "25%           5.000000\n",
      "50%         143.500000\n",
      "75%        6465.000000\n",
      "90%       34195.200000\n",
      "max      455626.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    \"\"\" This function takes a given string and split it into desired sized n-grams \"\"\"\n",
    "    return [text[i:i+n] for i in range(0,len(text),n)]\n",
    "\n",
    "# Test the ngrams function with a variety of ngrams\n",
    "test_string = \"I like chocolates\"\n",
    "print(\"Original: {}\".format(test_string))\n",
    "for i in list(range(3)):\n",
    "    print(\"\\t{}-grams: {}\".format(i+1, get_ngrams(test_string, i+1)))\n",
    "\n",
    "# Create a counter with the bi-grams\n",
    "ngrams = 2\n",
    "\n",
    "text = chain(*[get_ngrams(s, ngrams) for s in stories])\n",
    "cnt = Counter(text)\n",
    "\n",
    "# Create a pandas series with the counter results\n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "print(\"\\nSample of most-common bigrams\")\n",
    "print(freq_df.head(n=10))\n",
    "print(\"\\nMedian: {}\\n\".format(freq_df.median()))\n",
    "# Get summary statistics\n",
    "print(freq_df.describe(percentiles=[0.25,0.5,0.75,0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the size of the vocabulary\n",
    "\n",
    "We will set the vocabulary size to the number of words (bi-grams) that appear at least 10 times in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 734\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df>=10).sum()\n",
    "print(\"Size of vocabulary: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-grams to IDs: Defining a Keras tokenizer\n",
    "\n",
    "Here, we're going to fit a tokenizer on the train data in order to convert bi-grams to IDs. The tokenizer will assign a specific ID to each unique bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.1\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer for the determined vocabulary size\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "\n",
    "# Get ngrams in the training data\n",
    "train_ngram_stories = [get_ngrams(s,ngrams) for s in stories]\n",
    "# Fit the tokenizer\n",
    "tokenizer.fit_on_texts(train_ngram_stories)\n",
    "\n",
    "# Get the ID sequence for training data\n",
    "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
    "\n",
    "# Get the ID sequence for validation data\n",
    "val_ngram_stories = [get_ngrams(s,ngrams) for s in val_stories]\n",
    "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
    "\n",
    "# Get the ID sequence for testing data\n",
    "test_ngram_stories = [get_ngrams(s,ngrams) for s in test_stories]\n",
    "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at some word ID sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: _book_title_ : andrew_lang___the_yellow_fairy_book\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' a', 'nd', 're', 'w_', 'la', 'ng', '__', '_t', 'he', '_y', 'el', 'lo', 'w_', 'fa', 'ir', 'y_', 'bo', 'ok']\n",
      "Word ID sequence: [549, 97, 554, 100, 175, 537, 325, 7, 22, 25, 707, 112, 37, 522, 594, 4, 1, 81, 107, 707, 170, 133, 586, 161, 194]\n",
      "\n",
      "\n",
      "Original: _book_title_ : lewis_carroll___alice's_adventures_\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' l', 'ew', 'is', '_c', 'ar', 'ro', 'll', '__', '_a', 'li', 'ce', \"'s\", '_a', 'dv', 'en', 'tu', 're', 's_']\n",
      "Word ID sequence: [549, 97, 554, 100, 175, 537, 325, 53, 226, 49, 717, 52, 75, 54, 522, 1, 74, 120, 181, 1, 419, 39, 221, 25, 609]\n",
      "\n",
      "\n",
      "Original: _book_title_ : lucy_maud_montgomery___lucy_maud_mo\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' l', 'uc', 'y_', 'ma', 'ud', '_m', 'on', 'tg', 'om', 'er', 'y_', '__', 'lu', 'cy', '_m', 'au', 'd_', 'mo']\n",
      "Word ID sequence: [549, 97, 554, 100, 175, 537, 325, 53, 227, 586, 105, 298, 640, 43, 573, 102, 16, 586, 522, 289, 399, 640, 213, 672, 146]\n",
      "\n",
      "\n",
      "Original: _book_title_ : rudyard_kipling___the_jungle_book.t\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' r', 'ud', 'ya', 'rd', '_k', 'ip', 'li', 'ng', '__', '_t', 'he', '_j', 'un', 'gl', 'e_', 'bo', 'ok', '.t']\n",
      "Word ID sequence: [549, 97, 554, 100, 175, 537, 325, 82, 298, 394, 183, 1, 305, 74, 37, 522, 594, 4, 1, 119, 271, 537, 161, 194, 584]\n",
      "\n",
      "\n",
      "Original: _book_title_ : thornton_waldo_burgess___the_advent\n",
      "n-grams: ['_b', 'oo', 'k_', 'ti', 'tl', 'e_', ' :', ' t', 'ho', 'rn', 'to', 'n_', 'wa', 'ld', 'o_', 'bu', 'rg', 'es', 's_', '__', 'th', 'e_', 'ad', 've', 'nt']\n",
      "Word ID sequence: [549, 97, 554, 100, 175, 537, 325, 3, 73, 214, 35, 618, 66, 98, 639, 140, 311, 61, 609, 522, 6, 537, 76, 48, 79]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, tokens, seq in zip(test_stories[:5], test_ngram_stories[:5], test_data_seq[:5]):\n",
    "    print(\"Original: {}\".format(s[:50]))\n",
    "    print(\"n-grams: {}\".format(tokens[:25]))\n",
    "    print(\"Word ID sequence: {}\".format(seq[:25]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the TensorFlow `tf.data` pipeline\n",
    "\n",
    "Here we will define a `tf.data` pipeline to generate data for the model. In language modelling, data is generated as follows. Say you want to provide a `n` elements long sequence as the input to the model in order to generate text. Then you take a `n+1` long sequence `text` and split it into two parts; `text[:-1]` and `text[1:]`. At any step of the implementation, you can check the specification of the dataset with `print(tf.data.DatasetSpec.from_value(ds))`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.1\n",
    "\n",
    "# Code listing 10.3\n",
    "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
    "    \"\"\" Define a tf.data pipeline that takes a set of sequences of text and \n",
    "    convert them to fixed length sequences for the model \"\"\"\n",
    "    \n",
    "    # Define a tf.dataset from a ragged tensor created from data_seq\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq)) # tf.ragged.constant(data_seq)\n",
    "    \n",
    "    # If shuffle is set, shuffle the data (shuffle story order)\n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)\n",
    "    \n",
    "    # This function will create windows from data, given a window size and a shift\n",
    "    # Each window is a single entity    \n",
    "    \n",
    "    # windows function create neted dataset within text ds\n",
    "    # This is a special trick we use to unwrap those nested structures\n",
    "    #text_ds = text_ds.flat_map(lambda window: window.batch(n_seq+1, drop_remainder=True))    \n",
    "    text_ds = text_ds.flat_map(\n",
    "        lambda x: tf.data.Dataset.from_tensor_slices(\n",
    "            x\n",
    "        ).window(\n",
    "            n_seq+1, shift=shift\n",
    "        ).flat_map(\n",
    "            lambda window: window.batch(n_seq+1, drop_remainder=True)\n",
    "        )\n",
    "    ) \n",
    "    \n",
    "    # Shuffle the data (shuffle the order of n_seq+1 long sequences)\n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
    "    \n",
    "    # Batch the data\n",
    "    text_ds = text_ds.batch(batch_size)\n",
    "    \n",
    "    # Split each sequence to an input and a target\n",
    "    text_ds = tf.data.Dataset.zip(text_ds.map(lambda x: (x[:,:-1], x[:, 1:]))).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return text_ds    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some data\n",
    "\n",
    "Here you can see that `a` is a tuple with two Tensors; an input tensor and a target tensor. If you check the target tensor, each row in target is essentially a shift by 1 to the right of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 07:59:34.190073: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-27 07:59:34.190513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 07:59:34.190878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 07:59:34.191179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 07:59:34.606264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 07:59:34.606614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 07:59:34.606912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 07:59:34.607185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6393 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 15, 151,  85,  84,  30],\n",
      "       [325,  53, 227, 586, 105],\n",
      "       [175, 537, 325,  53, 227],\n",
      "       [ 87,   6,   2,  72,  76],\n",
      "       [  8,  49, 103,  22,  31],\n",
      "       [ 31,   7,  22,  11, 280]], dtype=int32)>, <tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[151,  85,  84,  30, 350],\n",
      "       [ 53, 227, 586, 105, 298],\n",
      "       [537, 325,  53, 227, 586],\n",
      "       [  6,   2,  72,  76,  86],\n",
      "       [ 49, 103,  22,  31,   7],\n",
      "       [  7,  22,  11, 280,  63]], dtype=int32)>)\n",
      "(<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 77,  23,  93, 205,  56],\n",
      "       [586, 105, 298, 640,  43],\n",
      "       [  2,  13, 122, 276, 488],\n",
      "       [  6, 537,  49, 112,  22],\n",
      "       [ 22,  31,   7,  22,  11],\n",
      "       [100, 175, 537, 325,  53]], dtype=int32)>, <tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 23,  93, 205,  56,  19],\n",
      "       [105, 298, 640,  43, 573],\n",
      "       [ 13, 122, 276, 488,  56],\n",
      "       [537,  49, 112,  22, 584],\n",
      "       [ 31,   7,  22,  11, 280],\n",
      "       [175, 537, 325,  53, 227]], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "ds = get_tf_pipeline(train_data_seq, 5, batch_size=6)\n",
    "\n",
    "for a in ds.take(2):\n",
    "\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and save hyperparameters so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_grams uses n=2\n",
      "Vocabulary size: 734\n",
      "Sequence length for model: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"n_grams uses n={}\".format(ngrams))\n",
    "print(\"Vocabulary size: {}\".format(n_vocab))\n",
    "\n",
    "n_seq=100\n",
    "print(\"Sequence length for model: {}\".format(n_seq))\n",
    "\n",
    "with open(os.path.join('models', 'text_hyperparams.pkl'), 'wb') as f:\n",
    "    pickle.dump({'n_vocab': n_vocab, 'ngrams':ngrams, 'n_seq': n_seq}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "Here we're going to define an embedding layer, a single LSTM layer and two dense layers. \n",
    "\n",
    "More on regularizing LSTM models: https://arxiv.org/pdf/1708.02182.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.2\n",
    "\n",
    "# Code listing 10.4\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512, input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True),\n",
    "    \n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    \n",
    "    # Defining a final Dense layer and softmax activation\n",
    "    tf.keras.layers.Dense(n_vocab, name='final_out'),\n",
    "    tf.keras.layers.Activation(activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Perplexity Metric\n",
    "\n",
    "Perplexity measures given a sequence of $n-1$ words, how surprised (or perplexed) the model was to see the $n^{th}$ word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.3\n",
    "\n",
    "# Code listing 10.5\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Inspired by https://gist.github.com/Gregorgeous/dbad1ec22efc250c76354d949a13cec3\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    \n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "      super().__init__(name=name, **kwargs)\n",
    "      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "      # The next 4 lines zero-out the padding from loss calculations, \n",
    "      # this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics \t\t\t      \n",
    "      loss_ = self.cross_entropy(real, pred)\n",
    "      \n",
    "      # Calculating the perplexity steps: \n",
    "      step1 = K.mean(loss_, axis=-1)\n",
    "      perplexity = K.exp(step1)\n",
    "      #perplexity = K.mean(step2)\n",
    "    \n",
    "      return perplexity \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):            \n",
    "      perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "      # Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations \n",
    "      super().update_state(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Perpelxity calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.2082006, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p = PerplexityMetric()\n",
    "# Define a set of true targets\n",
    "true = [[0, 1,2],[0, 1,2]]\n",
    "# Define a set of predictions\n",
    "pred = [[[0.9, 0.1, 0.0], [0.3, 0.7, 0.0], [0.0, 0.1, 0.9]],[[0.9, 0.1, 0.0], [0.3, 0.7, 0.0], [0.0, 0.1, 0.9]]]\n",
    "\n",
    "# Compute perplexity\n",
    "p.update_state(true, pred)\n",
    "print(p.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "We will compile the model with `sparse_categorical_crossentropy`, `adam` optimizer and `accuracy` and `perplexity` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 512)         376320    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 1024)        4724736   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 512)         524800    \n",
      "                                                                 \n",
      " final_out (Dense)           (None, None, 734)         376542    \n",
      "                                                                 \n",
      " activation (Activation)     (None, None, 734)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,002,398\n",
      "Trainable params: 6,002,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Here we're going to train the model. To keep the training shorter, we will only use 50/98 storings in the training set. We will also generate sequences at a shift of 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using metric=val_perplexity and mode=min for EarlyStopping\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 07:59:58.621601: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8400\n",
      "Could not load symbol cublasGetSmCountTarget from libcublas.so.11. Error: /usr/local/cuda-11.0/lib64/libcublas.so.11: undefined symbol: cublasGetSmCountTarget\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2936/2936 [==============================] - 257s 87ms/step - loss: 2.4087 - accuracy: 0.4273 - perplexity: 16.2584 - val_loss: 2.5031 - val_accuracy: 0.4140 - val_perplexity: 13.0831 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2936/2936 [==============================] - 262s 89ms/step - loss: 2.0756 - accuracy: 0.4840 - perplexity: 8.4849 - val_loss: 2.4731 - val_accuracy: 0.4141 - val_perplexity: 12.6376 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2936/2936 [==============================] - 264s 90ms/step - loss: 2.0465 - accuracy: 0.4887 - perplexity: 8.2267 - val_loss: 2.4717 - val_accuracy: 0.4204 - val_perplexity: 12.8945 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2936/2936 [==============================] - 260s 88ms/step - loss: 2.0539 - accuracy: 0.4860 - perplexity: 8.2592 - val_loss: 2.4379 - val_accuracy: 0.4220 - val_perplexity: 12.3238 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2936/2936 [==============================] - 263s 90ms/step - loss: 2.0510 - accuracy: 0.4855 - perplexity: 8.1996 - val_loss: 2.4304 - val_accuracy: 0.4213 - val_perplexity: 12.2095 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2936/2936 [==============================] - 264s 90ms/step - loss: 2.0502 - accuracy: 0.4850 - perplexity: 8.1865 - val_loss: 2.3914 - val_accuracy: 0.4283 - val_perplexity: 11.7027 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 2.0454 - accuracy: 0.4855 - perplexity: 8.1227 - val_loss: 2.4068 - val_accuracy: 0.4258 - val_perplexity: 12.0046 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 2.0476 - accuracy: 0.4845 - perplexity: 8.1466 - val_loss: 2.4468 - val_accuracy: 0.4178 - val_perplexity: 12.4371 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2936/2936 [==============================] - 261s 89ms/step - loss: 2.0934 - accuracy: 0.4709 - perplexity: 8.4191 - val_loss: 2.2503 - val_accuracy: 0.4533 - val_perplexity: 10.2302 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 2.0276 - accuracy: 0.4836 - perplexity: 7.8549 - val_loss: 2.2448 - val_accuracy: 0.4540 - val_perplexity: 10.1304 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 1.9966 - accuracy: 0.4900 - perplexity: 7.6005 - val_loss: 2.2337 - val_accuracy: 0.4564 - val_perplexity: 10.0757 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 1.9709 - accuracy: 0.4953 - perplexity: 7.3959 - val_loss: 2.2327 - val_accuracy: 0.4566 - val_perplexity: 10.0405 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "2936/2936 [==============================] - 265s 90ms/step - loss: 1.9495 - accuracy: 0.4999 - perplexity: 7.2364 - val_loss: 2.2341 - val_accuracy: 0.4580 - val_perplexity: 10.1032 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "2936/2936 [==============================] - 263s 89ms/step - loss: 1.9342 - accuracy: 0.5029 - perplexity: 7.1187 - val_loss: 2.2368 - val_accuracy: 0.4575 - val_perplexity: 10.1537 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 1.9328 - accuracy: 0.5021 - perplexity: 7.0919 - val_loss: 2.2103 - val_accuracy: 0.4649 - val_perplexity: 9.9038 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "2936/2936 [==============================] - 261s 89ms/step - loss: 1.9165 - accuracy: 0.5061 - perplexity: 6.9734 - val_loss: 2.2082 - val_accuracy: 0.4662 - val_perplexity: 9.9106 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 1.9091 - accuracy: 0.5078 - perplexity: 6.9237 - val_loss: 2.2200 - val_accuracy: 0.4626 - val_perplexity: 10.0065 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 1.9099 - accuracy: 0.5074 - perplexity: 6.9238 - val_loss: 2.2147 - val_accuracy: 0.4642 - val_perplexity: 9.9615 - lr: 1.0000e-06\n",
      "Epoch 19/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 1.9072 - accuracy: 0.5081 - perplexity: 6.9049 - val_loss: 2.2141 - val_accuracy: 0.4645 - val_perplexity: 9.9610 - lr: 1.0000e-06\n",
      "Epoch 20/50\n",
      "2936/2936 [==============================] - 260s 89ms/step - loss: 1.9062 - accuracy: 0.5083 - perplexity: 6.8979 - val_loss: 2.2139 - val_accuracy: 0.4646 - val_perplexity: 9.9590 - lr: 1.0000e-07\n",
      "It took 5223.01819396019 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "# Section 10.4\n",
    "\n",
    "train_ds = get_tf_pipeline(train_data_seq[:50], n_seq, shift=25, batch_size=128)\n",
    "valid_ds = get_tf_pipeline(val_data_seq, n_seq, shift=n_seq, batch_size=128)\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','1_language_modelling.log'))\n",
    "\n",
    "monitor_metric = 'val_perplexity'\n",
    "mode = 'min' \n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "# This function keeps the initial learning rate for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def scheduler(epoch, lr):  \n",
    "    if epoch==0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "#lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=2, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=5, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, epochs=50, \n",
    "          validation_data = valid_ds,\n",
    "          callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 2s 34ms/step - loss: 2.2949 - accuracy: 0.4553 - perplexity: 11.2146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2948660850524902, 0.4552602171897888, 11.214558601379395]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "test_ds = get_tf_pipeline(test_data_seq, n_seq, shift=n_seq, batch_size=batch_size)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(model, os.path.join('models', '2_gram_lm.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'text_hyperparams.pkl'), 'rb') as f:\n",
    "    hparams = pickle.load(f)\n",
    "\n",
    "ngrams = hparams['ngrams']\n",
    "n_vocab = hparams[\"n_vocab\"]\n",
    "n_seq = hparams[\"n_seq\"]\n",
    "\n",
    "model = tf.keras.models.load_model(os.path.join('models', '2_gram_lm.h5'), compile=False)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inference model (Functional API)\n",
    "\n",
    "Here, we're going to define an inference model. We need to actually define a new model with identical weights to the original but will make changes to inputs and outputs. Essentially, we will define a model to which we can pass in an intial state (hidden state of GRU) and outputs the final prediction as well as the new hidden state.\n",
    "\n",
    "This way we can recursively call our model on new predictions to generate a story for any number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 512)    376320      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1024)]       0           []                               \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    [(None, None, 1024)  4724736     ['embedding_1[0][0]',            \n",
      "                                , (None, 1024)]                   'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 512)    524800      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " final_out (Dense)              (None, None, 734)    376542      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, None, 734)    0           ['final_out[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,002,398\n",
      "Trainable params: 6,002,398\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 10.5\n",
    "\n",
    "# Code listing 10.6\n",
    "\n",
    "# Define inputs to the model\n",
    "inp = tf.keras.layers.Input(shape=(None,))\n",
    "inp_state = tf.keras.layers.Input(shape=(1024,))\n",
    "\n",
    "# Define embedding layer and output\n",
    "emb_layer = tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512, input_shape=(None,))\n",
    "emb_out = emb_layer(inp)\n",
    "\n",
    "# Defining a GRU layer and output\n",
    "gru_layer = tf.keras.layers.GRU(1024, return_state=True, return_sequences=True)\n",
    "gru_out, gru_state = gru_layer(emb_out, initial_state=inp_state)\n",
    "\n",
    "# Defining a Dense layer and output\n",
    "dense_layer = tf.keras.layers.Dense(512, activation='relu')\n",
    "dense_out = dense_layer(gru_out)\n",
    "\n",
    "# Defining the final Dense layer and output\n",
    "final_layer = tf.keras.layers.Dense(n_vocab, name='final_out')\n",
    "final_out = final_layer(dense_out)\n",
    "softmax_out = tf.keras.layers.Activation(activation='softmax')(final_out)\n",
    "\n",
    "# Define final model\n",
    "infer_model = tf.keras.models.Model(inputs=[inp, inp_state], outputs=[softmax_out, gru_state])\n",
    "\n",
    "# Copy the weights from the original model\n",
    "emb_layer.set_weights(model.get_layer('embedding').get_weights())\n",
    "gru_layer.set_weights(model.get_layer('gru').get_weights())\n",
    "dense_layer.set_weights(model.get_layer('dense').get_weights())\n",
    "final_layer.set_weights(model.get_layer('final_out').get_weights())\n",
    "\n",
    "# Summary\n",
    "infer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text with greedy decoding\n",
    "\n",
    "Here we will generate text with the simplest approach we can think of. At time $t=1$, we start with a predefined sequence, and feed that to `infer_model`. At the end of the sequence we get $w_1$ (the prediction at $t=1$). $w_1$ will be the input to the model at $t=2$ and the model will generate $w_2$ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions from a 54 element long input\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final text: \n",
      "chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank , and then they went to the station .\n",
      " `` it 's all right , '' said mrs jo to herself , `` and you 'llitte , and i 'll be able to see your father 's .\n",
      " i 'm not goin ' to the house , anyhow , '' said the story girls .\n",
      " `` i 'm not going to be something in the world , '' said mrs. march , who wanted to be as much as her own .\n",
      " `` i 'm sorry to be able to say that , '' said the princess , `` any more than yours , and i 'lld thee the most beautiful sea ... and i 'm going to bring you a little while , and i 'll be all right .\n",
      " i 'm sure i shall be able to say that , if you will be ablemant to be done .\n",
      " i 'm sure i shall be some way to the sea .\n",
      " i 'm surprised they were all there .\n",
      " i 'm sure i shall be able-tone , '' said mrs. march , who was so sorry to be ablazed at the door of the way .\n",
      " `` what are you doing there ? ''\n",
      " asked the prince ; ` and i 'm going to be a strangers about the sunshine , and the little girls were always a little .\n",
      " i 'm no longeries of the worldlyways .\n",
      " i 'm not \n"
     ]
    }
   ],
   "source": [
    "# Section 10.5\n",
    "\n",
    "text = get_ngrams(\n",
    "    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank ,\".lower(), \n",
    "    ngrams\n",
    ")\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# build up model state using the given string\n",
    "print(\"Making predictions from a {} element long input\".format(len(seq[0])))\n",
    "\n",
    "# Reset the state of the model initially\n",
    "model.reset_states()\n",
    "# Definin the initial state as all zeros\n",
    "state = np.zeros(shape=(1,1024))\n",
    "# Recursively update the model by assining new state to state\n",
    "for c in seq[0]:    \n",
    "    out, state = infer_model.predict([np.array([[c]]), state], verbose=0)\n",
    "\n",
    "# Get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = tokenizer.index_word[wid]\n",
    "text.append(word)\n",
    "\n",
    "# Define first input to generate text recursively from\n",
    "x = np.array([[wid]])\n",
    "\n",
    "# Code listing 10.7\n",
    "for _ in range(500):\n",
    "    \n",
    "    # Get the next output and state\n",
    "    out, state = infer_model.predict([x, state], verbose=0)\n",
    "    \n",
    "    # Get the word id and the word from out\n",
    "    out_argsort = np.argsort(out[0], axis=-1).ravel()        \n",
    "    wid = int(out_argsort[-1])\n",
    "    word = tokenizer.index_word[wid]\n",
    "    \n",
    "    # If the word ends with space, we introduce a bit of randomness\n",
    "    # Essentially pick one of the top 3 outputs for that timestep depending on their likelihood\n",
    "    if word.endswith(' '):\n",
    "        if np.random.normal()>0.5:\n",
    "            width = 3\n",
    "            i = np.random.choice(list(range(-width,0)), p=out_argsort[-width:]/out_argsort[-width:].sum())    \n",
    "            wid = int(out_argsort[i])    \n",
    "            word = tokenizer.index_word[wid]\n",
    "            \n",
    "    # Append the prediction\n",
    "    text.append(word)\n",
    "    \n",
    "    # Recursively make the current prediction the next input\n",
    "    x = np.array([[wid]])\n",
    "    \n",
    "# Print the final output    \n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search decoding\n",
    "\n",
    "Beam search is a more sophisticated and better decoding technique. In beam search we predict several timesteps in to the future and pick the sequence that gives the best joint probability. Remember that, in greedy decoding we only predicted 1 step into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the beam search logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10.6\n",
    "\n",
    "# Code listing 10.8\n",
    "\n",
    "def beam_one_step(model, input_, state):    \n",
    "    \"\"\" Perform the model update and output for one step\"\"\"\n",
    "    output, new_state = model.predict([input_, state], verbose=0)\n",
    "    return output, new_state\n",
    "\n",
    "\n",
    "def beam_search(model, input_, state, beam_depth=5, beam_width=3, ignore_blank=True):\n",
    "    \"\"\" Defines an outer wrapper for the computational function of beam search \"\"\"\n",
    "    \n",
    "    def recursive_fn(input_, state, sequence, log_prob, i):\n",
    "        \"\"\" This function performs actual recursive computation of the long string\"\"\"\n",
    "        \n",
    "        if i == beam_depth:\n",
    "            \"\"\" Base case: Terminate the beam search \"\"\"\n",
    "            results.append((list(sequence), state, np.exp(log_prob)))            \n",
    "            return sequence, log_prob, state\n",
    "        else:\n",
    "            \"\"\" Recursive case: Keep computing the output using the previous outputs\"\"\"\n",
    "            output, new_state = beam_one_step(model, input_, state)\n",
    "            \n",
    "            # Get the top beam_widht candidates for the given depth\n",
    "            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)\n",
    "            top_probs, top_ids = top_probs.numpy().ravel(), top_ids.numpy().ravel()\n",
    "            \n",
    "            # For each candidate compute the next prediction\n",
    "            for p, wid in zip(top_probs, top_ids):                \n",
    "                new_log_prob = log_prob + np.log(p)\n",
    "                \n",
    "                # we are going to penalize joint probability whenever the same symbol is repeating\n",
    "                if len(sequence)>0 and wid == sequence[-1]:\n",
    "                    new_log_prob = new_log_prob + np.log(1e-1)\n",
    "                    \n",
    "                sequence.append(wid)                \n",
    "                _ = recursive_fn(np.array([[wid]]), new_state, sequence, new_log_prob, i+1)                                         \n",
    "                sequence.pop()\n",
    "        \n",
    "    \n",
    "    results = []\n",
    "    sequence = []\n",
    "    log_prob = 0.0\n",
    "    recursive_fn(input_, state, sequence, log_prob, 0)    \n",
    "\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the actual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making 54 predictions from input\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final text: \n",
      "chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank , and they were sitting on the grass , and then there was a great deal of trouble .\n",
      " the princess was sitting on the grass , and her eyes were standing at the door of the window , and then they went on again , and then he went into the house and there was nothing to do with them .\n",
      " `` what is it ? ''\n",
      " said mrs. jo , smiling .\n",
      " `` it 's all right , and i 'm going to tell you that you will never be afraid of that , '' said the king .\n",
      " `` i 'll tell you what i want .\n",
      " there is nothing else to do , '' answered mrs. jo , smiling at the entrance of the world .\n",
      " `` i do n't want any more , and i 'm going to tell you that you were not going to be there , '' said mrs. jo .\n",
      " `` i do n't know what to do .\n",
      " i 'm going to teach you to see that you will never be the best thing to do with you . ''\n",
      " `` i 'm not going to tell you , '' she said , with a sigh .\n",
      " `` i do n't want to go to the world . ''\n",
      " `` i 'm so glad to me , '' said marilla .\n",
      " `` i 'm going to bed , '' said marilla .\n",
      " `` what is it ? ''\n",
      " said the story girl .\n",
      " `` i 'm sure i 'm going to see that there 's nothing to do with you . ''\n",
      " then the king opened her eyes , and said , `` i 'll tell you where i wanted to be . ''\n",
      " `` i do n't know , '' she answered , with a sigh of relief .\n",
      " `` i do n't know , '' said mrs. march .\n",
      " `` i 'm going to tell you what you 've heard of it , and i 'm going to tell you that you will never be able to s\n"
     ]
    }
   ],
   "source": [
    "# Section 10.6\n",
    "\n",
    "# Code listing 10.9\n",
    "\n",
    "text = get_ngrams(\n",
    "    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank ,\".lower(),     \n",
    "    ngrams\n",
    ")\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# build up model state using the given string\n",
    "print(\"Making {} predictions from input\".format(len(seq[0])))\n",
    "\n",
    "#model.reset_states()\n",
    "state = np.zeros(shape=(1,1024))\n",
    "for c in seq[0]:    \n",
    "    out, state = infer_model.predict([np.array([[c]]), state], verbose=0)\n",
    "\n",
    "# get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = tokenizer.index_word[wid]\n",
    "text.append(word)\n",
    "\n",
    "x = np.array([[wid]])\n",
    "\n",
    "# Predict for 100 time steps\n",
    "for i in range(100):    \n",
    "    \n",
    "    # Get the results from beam search\n",
    "    result = beam_search(infer_model, x, state, 7, 2)\n",
    "    \n",
    "    # Get one of the top 10 results based on their likelihood\n",
    "    n_probs = np.array([p for _,_,p in result[:10]])\n",
    "    p_j = np.random.choice(list(range(n_probs.size)), p=n_probs/n_probs.sum())                    \n",
    "    best_beam_ids, state, _ = result[p_j]\n",
    "    x = np.array([[best_beam_ids[-1]]])\n",
    "            \n",
    "    text.extend([tokenizer.index_word[w] for w in best_beam_ids])    \n",
    "\n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
