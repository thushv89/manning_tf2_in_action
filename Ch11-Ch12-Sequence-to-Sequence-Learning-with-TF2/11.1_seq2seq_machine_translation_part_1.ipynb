{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq models (Sequence-to-Sequence)\n",
    "\n",
    "Sequence to sequence models are a variant of deep learning models that consists of an encoder and a decoder. They are used for problems that map an abitrarily long sequence to another arbitrarliy long sequence. For example, in machine translation, you convert a sequence of words in a source language to a sequence of words in a target language. Here we will see how we can use a seq2seq model to solve a machine translation task to convert English to German.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch11-Ch12-Sequence-to-Sequence-Learning-with-TF2/11.1_seq2seq_machine_translation_part_1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    " \n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.manythings.org/anki/\n",
    "    \n",
    "german-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# Not setting this led to the following error\n",
    "# _Derived_]RecvAsync is cancelled.   \n",
    "# [[{{node gradient_tape/model_1/embedding_1/embedding_lookup/Reshape/_172}}]] [Op:__inference_train_function_31985]\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (Requires manual download)\n",
    "\n",
    "Unfortunately, this dataset **must be manually downloaded** by clicking [this link](http://www.manythings.org/anki/deu-eng.zip). Then place the downloaded `deu-eng.zip` file in the `Ch11/data` folder before running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Section 11.1\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Make sure the zip file has been downloaded\n",
    "if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
    "    raise FileNotFoundError(\n",
    "        \"Uh oh! Did you download the deu-eng.zip from http://www.manythings.org/anki/deu-eng.zip manually and place it in the Ch11/data folder?\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
    "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "    else:\n",
    "        print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "Data is in a single `.txt` file. It is a parallel corpus meaning there is a English sentence/phrase/paragraph and a corresponding German translation of it side-by-side. In the file, the source input and the translation are separated by a tab (i.e. tab-seperated file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (242586, 2)\n"
     ]
    }
   ],
   "source": [
    "# Section 11.1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', encoding='utf-8', encoding_errors=\"strict\", header=None)\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "print('df.shape = {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are \\xc2\\xa0 (undecode-able bytes remaining in some text)\n",
    "# This can cause errors like UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 3: unexpected end of data\n",
    "# when using the TextVectorization layer\n",
    "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
    "\n",
    "df = df.iloc[clean_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242577</th>\n",
       "      <td>Even if some sentences by non-native speakers ...</td>\n",
       "      <td>Auch wenn Sätze von Nichtmuttersprachlern mitu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242578</th>\n",
       "      <td>Remember that the purpose of the Tatoeba Proje...</td>\n",
       "      <td>Es gilt zu bedenken, dass es das Anliegen des ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242579</th>\n",
       "      <td>It's so easy to write good example sentences, ...</td>\n",
       "      <td>Es ist so leicht, gute Beispielsätze zu schrei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242580</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand, der deine Herkunft nicht kennt, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242581</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand Fremdes dir sagt, dass du dich wie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       EN  \\\n",
       "242577  Even if some sentences by non-native speakers ...   \n",
       "242578  Remember that the purpose of the Tatoeba Proje...   \n",
       "242579  It's so easy to write good example sentences, ...   \n",
       "242580  If someone who doesn't know your background sa...   \n",
       "242581  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                                       DE  \n",
       "242577  Auch wenn Sätze von Nichtmuttersprachlern mitu...  \n",
       "242578  Es gilt zu bedenken, dass es das Anliegen des ...  \n",
       "242579  Es ist so leicht, gute Beispielsätze zu schrei...  \n",
       "242580  Wenn jemand, der deine Herkunft nicht kennt, s...  \n",
       "242581  Wenn jemand Fremdes dir sagt, dass du dich wie...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a smaller sample for computational speed\n",
    "\n",
    "There are more than 220000 samples in the original dataset. We will be using a smaller set of 50000 for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "df = df.sample(n=n_samples, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the `SOS` and `EOS` tokens (Decoder)\n",
    "\n",
    "We will add these special tokens to the translated targets. `sos` indicates the start of the sentence and `eos` marks the end of the sentence. \n",
    "\n",
    "E.g. `Grüß Gott!` becomes `sos Grüß Gott! eos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training/validation/testing data\n",
    "\n",
    "We will be creating three datasets by sampling randomly (without replacement);\n",
    "\n",
    "* Test dataset - 5000 samples\n",
    "* Validation dataset - 5000 samples\n",
    "* Training dataset - 40000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape = (5000, 2)\n",
      "valid_df.shape = (5000, 2)\n",
      "train_df.shape = (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
    "\n",
    "print('test_df.shape = {}'.format(test_df.shape))\n",
    "print('valid_df.shape = {}'.format(valid_df.shape))\n",
    "print('train_df.shape = {}'.format(train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary sizes (English and German)\n",
    "\n",
    "Calculate the vocabulary size. We will only consider the words that appear at least 10 times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9562\n",
      "to     8729\n",
      "I      8427\n",
      "the    6880\n",
      "you    5947\n",
      "a      5678\n",
      "is     4362\n",
      "in     2617\n",
      "of     2508\n",
      "was    2253\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2188\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "sos      40000\n",
      "eos      40000\n",
      "Tom      10038\n",
      "Ich       7956\n",
      "ist       4753\n",
      "nicht     4690\n",
      "zu        3594\n",
      "Sie       3459\n",
      "du        3017\n",
      "das       2934\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2479\n"
     ]
    }
   ],
   "source": [
    "# Section 11.1\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Create a flattened list from English words\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "# Create a flattened list of German words\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "\n",
    "# Get the vocabulary size of words appearing more than or equal to 10 times\n",
    "n=10\n",
    "\n",
    "# Code listing 11.1\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \n",
    "    \"\"\" Get the vocabulary size above a certain threshold \"\"\"\n",
    "    \n",
    "    # Generate a counter object i.e. dict word -> frequency\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    # Create a pandas series from the counter, then sort most frequent to least\n",
    "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        # Print most common words\n",
    "        print(freq_df.head(n=10))\n",
    "\n",
    "    # Count of words >= n frequent    \n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "        \n",
    "    return n_vocab\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the sequence length (English and German)\n",
    "\n",
    "Here we compute the sequence length of the sequences in the English and German corpora. To ignore the outliers, we only consider data between the 1% and 99% quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.300925\n",
      "std          2.570960\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         44.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39548.000000\n",
      "mean         6.182942\n",
      "std          2.301208\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          7.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         8.337050\n",
      "std          2.562335\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         42.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39232.000000\n",
      "mean         8.253161\n",
      "std          2.260831\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%          9.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Section 11.1\n",
    "\n",
    "# Code listing 11.2\n",
    "def print_sequence_length(str_ser):\n",
    "    \n",
    "    \"\"\" Print the summary stats of the sequence length \"\"\"\n",
    "    \n",
    "    # Create a pd.Series, which contain the sequence length for each review\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "\n",
    "    # Get the median as well as summary statistics of the sequence length\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    # Get the quantiles at given marks\n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    # Print the summary stats of the data between the defined quantlies\n",
    "    print(seq_length_ser[(seq_length_ser >= p_01) & (seq_length_ser < p_99)].describe())\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the vocabulary size and sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2188\n",
      "DE vocabulary size: 2479\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow `TextVectorization` layer\n",
    "\n",
    "The `TextVectorization` layer takes in strings and convert them to token IDs. The layer can build a vocabulary using a given text corups and uses that to generate the token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the vectorization layer for English\n",
      "Fitting the EN vectorization layer on data\n",
      "\tDone\n",
      "\n",
      "Defined the vectorization layer for German\n",
      "Fitting the DE vectorization layer on data\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "# Section 11.2\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "print(\"Defined the vectorization layer for English\")\n",
    "\n",
    "# Create the text vectorization layer (English)\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None\n",
    ")\n",
    "\n",
    "print(\"Fitting the EN vectorization layer on data\")\n",
    "# Here we are calling adapt to fit the vectorization layer with text\n",
    "# so that it learns the vocabulary\n",
    "en_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n",
    "print(\"\\tDone\")\n",
    "\n",
    "print(\"\\nDefined the vectorization layer for German\")\n",
    "\n",
    "# Create the text vectorization layer (German)\n",
    "de_vectorize_layer = TextVectorization(\n",
    "    max_tokens=de_vocab,    \n",
    "    output_mode='int',\n",
    "    output_sequence_length=de_seq_length,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "print(\"Fitting the DE vectorization layer on data\")\n",
    "de_vectorize_layer.adapt(np.array(train_df[\"DE\"].tolist()))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TextVectorization` layer in action\n",
    " \n",
    "### How to use the layer (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "Input data: \n",
      "[['run'], [\"I'll go home\"], ['ectoplasmic residue']]\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[436   0   0]\n",
      " [ 87  49 115]\n",
      " [  1   1   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(en_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, \n",
    "input_data = [[\"run\"], [\"I\\'ll go home\"],[\"ectoplasmic residue\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the layer (DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "Input data: \n",
      "[['[sos] Geh'], ['geh lauf']]\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[  2 610   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [610   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(de_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, \n",
    "input_data = [[\"[sos] Geh\"], [\"geh lauf\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of the vocabulary\n",
    "\n",
    "Let's print some words from the two vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "['', '[UNK]', 'tom', 'to', 'you', 'the', 'i', 'a', 'is', 'that']\n",
      "2188\n",
      "\n",
      "German\n",
      "None\n",
      "['', '[UNK]', 'sos', 'eos', 'ich', 'tom', 'nicht', 'ist', 'das', 'du']\n",
      "2479\n"
     ]
    }
   ],
   "source": [
    "# Section 11.2\n",
    "\n",
    "print(\"English\")\n",
    "# Print first few words in the vocabulary\n",
    "print(en_vectorize_layer.get_vocabulary()[:10])\n",
    "# Print the size of the vocabulary\n",
    "print(len(en_vectorize_layer.get_vocabulary()))\n",
    "\n",
    "print(\"\\nGerman\")\n",
    "# Print first few words in the vocabulary\n",
    "print(de_vectorize_layer._lookup_layer.input_vocabulary)\n",
    "print(de_vectorize_layer.get_vocabulary()[:10])\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print(len(de_vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Seq2Seq model\n",
    "\n",
    "Here we define an encoder decoder model to translate between English and German. We will be using a bidirectional encoder and a standard decoder. The model will use Gated Recurrent Unit (GRU) as the recurrent component. The encoder and the decoder has their own `TextVectorization` layers as they use two different languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.2\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Code listing 11.3\n",
    "def get_vectorizer(corpus, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
    "    \n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    \n",
    "    # Definie an input layer that takes a list of strings (or an array of strings)\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    # When defining the vocab size, we'd add two for special tokens '' (Padding) and '[UNK]' (Oov tokens)\n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,                \n",
    "    )\n",
    "    \n",
    "    # Fit the vectorizer layer on the data\n",
    "    vectorize_layer.adapt(corpus)\n",
    "        \n",
    "    # Get the token IDs\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "        \n",
    "    if not return_vocabulary: \n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name)    \n",
    "    else:\n",
    "        # Returns the vocabulary in addition to the model\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()\n",
    "    \n",
    "# Code listing 11.4   \n",
    "def get_encoder(n_vocab, vectorizer):\n",
    "    \"\"\" Define the encoder of the seq2seq model\"\"\"\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "\n",
    "    # Vectorize the data (assign token IDs)\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='e_embedding')\n",
    "    # Get the embeddings of the token IDs\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Define a bidirectional GRU layer\n",
    "    # Encoder looks at the english text (i.e. the input) both backwards and forward\n",
    "    # this leads to better performance\n",
    "    gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, name='e_gru'), name='e_bidirectional_gru')\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    gru_out = gru_layer(emb_out)\n",
    "    \n",
    "    # Define the encoder model\n",
    "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out, name='encoder')\n",
    "        \n",
    "    return encoder\n",
    "\n",
    "\n",
    "# Code listing 11.5\n",
    "def get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n",
    "    \"\"\" Define the final encoder-decoder model \"\"\"\n",
    "    \n",
    "    # Encoder's input\n",
    "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')    \n",
    "    # Get the encoders final output\n",
    "    d_init_state = encoder(e_inp)\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    # This input layer is used to train the seq2seq model with teacher-forcing\n",
    "    # we feed the German sequence as the input and ask the model to predict \n",
    "    # it with the words offset by 1 (i.e. next word)\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    \n",
    "    # Vectorize the data (assign token IDs)\n",
    "    d_vectorized_out = vectorizer(d_inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    # Note that this is a different embedding layer to the encoder's embedding layer\n",
    "    d_emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='d_embedding')\n",
    "    \n",
    "    # Get the embeddings of the token IDs\n",
    "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
    "    \n",
    "    # Define a GRU layer\n",
    "    # Unlike the encoder, we cannot define a bidirectional GRU for the decoder\n",
    "    # Why?\n",
    "    d_gru_layer = tf.keras.layers.GRU(256, return_sequences=True, name='d_gru')\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)\n",
    "    \n",
    "    # Define an intermediate dense layer\n",
    "    d_dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')\n",
    "    d_dense1_out = d_dense_layer_1(d_gru_out)\n",
    "    \n",
    "    # The final prediction layer with softmax\n",
    "    d_dense_layer_final = tf.keras.layers.Dense(n_vocab+2, activation='softmax', name='d_dense_final')\n",
    "    d_final_out = d_dense_layer_final(d_dense1_out)\n",
    "    \n",
    "    # Define the full model\n",
    "    seq2seq = tf.keras.models.Model(inputs=[e_inp, d_inp], outputs=d_final_out, name='final_seq2seq')\n",
    "    \n",
    "    return seq2seq\n",
    "\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(np.array(train_df[\"EN\"].tolist()), en_vocab, max_length=en_seq_length, name='e_vectorizer')\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(np.array(train_df[\"DE\"].tolist()), de_vocab, max_length=de_seq_length-1, name='d_vectorizer')\n",
    "\n",
    "# Define the final model\n",
    "encoder = get_encoder(en_vocab, en_vectorizer)\n",
    "final_model = get_final_seq2seq_model(de_vocab, encoder, de_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "Compile the model with a suitable loss, an optimizer and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"final_seq2seq\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " d_input (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " d_vectorizer (Functional)      (None, 20)           0           ['d_input[0][0]']                \n",
      "                                                                                                  \n",
      " e_input_final (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " d_embedding (Embedding)        (None, 20, 128)      317568      ['d_vectorizer[0][0]']           \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, 256)          478464      ['e_input_final[0][0]']          \n",
      "                                                                                                  \n",
      " d_gru (GRU)                    (None, 20, 256)      296448      ['d_embedding[0][0]',            \n",
      "                                                                  'encoder[0][0]']                \n",
      "                                                                                                  \n",
      " d_dense_1 (Dense)              (None, 20, 512)      131584      ['d_gru[0][0]']                  \n",
      "                                                                                                  \n",
      " d_dense_final (Dense)          (None, 20, 2481)     1272753     ['d_dense_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,496,817\n",
      "Trainable params: 2,496,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 11.2\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Compile the model\n",
    "final_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating MT models - BLEU metric\n",
    "\n",
    "In machine translation, a popular choice for assessing performance is the BiLingual Evaluation Understudy (BLEU) metric. Word-to-word accuracy does not reflect the true performance of these models as there can be different ways the same phrase can be translated to. BLEU can take into account such multiple translations when computing the final score. Furthermore, BLEU is superior because it measures precision at multiple n-gram scales between the actual and predicted translations.\n",
    "\n",
    "The implementation is inspired by: https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py\n",
    "\n",
    "### Defining the BLEU metric\n",
    "\n",
    "Below we define a `BLEUMetric` object that can be used to compute the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.3\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from bleu import compute_bleu\n",
    "\n",
    "# Code listing 11.8\n",
    "class BLEUMetric(object):\n",
    "    \n",
    "    def __init__(self, vocabulary, name='perplexity', **kwargs):\n",
    "      \"\"\" Computes the BLEU score (Metric for machine translation) \"\"\"\n",
    "      super().__init__()\n",
    "      self.vocab = vocabulary\n",
    "      self.id_to_token_layer = StringLookup(vocabulary=self.vocab, num_oov_indices=0, oov_token=\"[KNU]\", invert=True)\n",
    "      print(self.id_to_token_layer.get_vocabulary()[:100])\n",
    "    \n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\" Calculate the BLEU score for targets and predictions \"\"\"\n",
    "        \n",
    "        # Get the predicted token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  \n",
    "        \n",
    "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "        \n",
    "        def clean_text(tokens):\n",
    "            \n",
    "            \"\"\" Clean padding and [SOS]/[EOS] tokens to only keep meaningful words \"\"\"\n",
    "            \n",
    "            # 3. Strip the string of any extra white spaces\n",
    "            translations_in_bytes = tf.strings.strip(\n",
    "                        # 2. Replace everything after the eos token with blank\n",
    "                        tf.strings.regex_replace(\n",
    "                            # 1. Join all the tokens to one string in each sequence\n",
    "                            tf.strings.join(\n",
    "                                tf.transpose(tokens), separator=' '\n",
    "                            ),\n",
    "                        \"eos.*\", \"\"),\n",
    "                   )\n",
    "            \n",
    "            # Decode the byte stream to a string\n",
    "            translations = np.char.decode(\n",
    "                translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8'\n",
    "            )\n",
    "            \n",
    "            # If the string is empty, add a [UNK] token\n",
    "            # Otherwise get a Division by zero error\n",
    "            translations = [sent if len(sent)>0 else '[UNK]' for sent in translations ]\n",
    "            \n",
    "            # Split the sequences to individual tokens \n",
    "            translations = np.char.split(translations).tolist()\n",
    "            \n",
    "            return translations\n",
    "        \n",
    "        # Get the clean versions of the predictions and real seuqences\n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
    "        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n",
    "\n",
    "        # The compute_bleu method accpets the translations and references in the following format\n",
    "        # tranlation - list of list of tokens\n",
    "        # references - list of list of list of tokens\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
    "\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the BLEU metric\n",
    "\n",
    "Below you can see BLEU being used to computer the similarity between a translation (predicted) and reference (true target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score with longer correctly predicte phrases: 0.7598356856515925\n",
      "BLEU score without longer correctly predicte phrases: 0.537284965911771\n"
     ]
    }
   ],
   "source": [
    "translation = [['[UNK]', '[UNK]', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "bleu1, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "translation = [['[UNK]', 'einmal', 'mÃssen', '[UNK]', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "\n",
    "bleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "print(\"BLEU score with longer correctly predicte phrases: {}\".format(bleu1))\n",
    "print(\"BLEU score without longer correctly predicte phrases: {}\".format(bleu2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with a custom loop\n",
    "\n",
    "We will train the model using a custom loop as we want to incorporate BLEU as a metric in our training. We will follow the following procedure;\n",
    "\n",
    "* Each epoch,\n",
    "  * Shuffle the training data\n",
    "  * Train our model on all the training data (in batches)\n",
    "  * Evaluate the model on validation data\n",
    "* Finally, evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.3\n",
    "import time\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 8 #128\n",
    "\n",
    "# Code listing 11.6\n",
    "def prepare_data(train_df, valid_df, test_df):\n",
    "    \"\"\" Create a data dictionary from the dataframes containing data \"\"\"\n",
    "    \n",
    "    data_dict = {}\n",
    "    for label, df in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df]):\n",
    "        en_inputs = np.array(df[\"EN\"].tolist())\n",
    "        de_inputs = np.array(df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist())\n",
    "        de_labels = np.array(df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist())\n",
    "        data_dict[label] = {'encoder_inputs': en_inputs, 'decoder_inputs': de_inputs, 'decoder_labels': de_labels}\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# Code listing 11.7\n",
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None): \n",
    "    \"\"\" Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n",
    "        \n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed create a shuffling automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "    \n",
    "    # Return shuffled data\n",
    "    return (en_inputs[shuffle_inds], de_inputs[shuffle_inds], de_labels[shuffle_inds]), shuffle_inds\n",
    "\n",
    "\n",
    "# Code listing 11.9\n",
    "def evaluate_model(model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, batch_size):\n",
    "    \"\"\" Evaluate the model on various metrics such as loss, accuracy and BLEU \"\"\"\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    \n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    # Get the number of batches\n",
    "    n_batches = en_inputs_raw.shape[0]//batch_size\n",
    "    print(\" \", end='\\r')\n",
    "\n",
    "    # Evaluate one batch at a time\n",
    "    for i in range(n_batches):\n",
    "        # Status update\n",
    "        print(\"Evaluating batch {}/{}\".format(i+1, n_batches), end='\\r')\n",
    "\n",
    "        # Get the inputs and targers\n",
    "        x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "        y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "        # Get the evaluation metrics\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        # Get the predictions to compute BLEU\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Update logs\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "    \n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)\n",
    "    \n",
    "\n",
    "# Code listing 11.10\n",
    "def train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n",
    "    \"\"\" Training the model and evaluating on validation/test sets \"\"\"\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "\n",
    "    shuffle_inds = None\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        bleu_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         Train Phase                                 #\n",
    "        # =================================================================== #\n",
    "\n",
    "        # Shuffle data at the beginning of every epoch\n",
    "        (en_inputs_raw,de_inputs_raw,de_labels_raw), shuffle_inds  = shuffle_data(\n",
    "            data_dict['train']['encoder_inputs'],\n",
    "            data_dict['train']['decoder_inputs'],\n",
    "            data_dict['train']['decoder_labels'],\n",
    "            shuffle_inds\n",
    "        )\n",
    "\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0]//batch_size\n",
    "\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(\"Training batch {}/{}\".format(i+1, n_train_batches), end='\\r')\n",
    "\n",
    "            # Get a batch of inputs (english and german sequences)\n",
    "            x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "            # Get a batch of targets (german sequences offset by 1)\n",
    "            y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)        \n",
    "            # Evaluate the model to get the metrics\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "            # Get the final prediction to compute BLEU\n",
    "            pred_y = model.predict(x, verbose=0)\n",
    "\n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "            bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                      Validation Phase                               #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        val_en_inputs = data_dict['valid']['encoder_inputs']\n",
    "        val_de_inputs = data_dict['valid']['decoder_inputs']\n",
    "        val_de_labels = data_dict['valid']['decoder_labels']\n",
    "            \n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model, vectorizer, val_en_inputs, val_de_inputs, val_de_labels, batch_size\n",
    "        )\n",
    "            \n",
    "        # Print the evaluation metrics of each epoch\n",
    "        print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
    "        print(\"\\t(train) loss: {} - accuracy: {} - bleu: {}\".format(np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)))\n",
    "        print(\"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(val_loss, val_accuracy, val_bleu))\n",
    "    \n",
    "    # =================================================================== #\n",
    "    #                      Test Phase                                     #\n",
    "    # =================================================================== #    \n",
    "    \n",
    "    test_en_inputs = data_dict['test']['encoder_inputs']\n",
    "    test_de_inputs = data_dict['test']['decoder_inputs']\n",
    "    test_de_labels = data_dict['test']['decoder_labels']\n",
    "            \n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "            model, vectorizer, test_en_inputs, test_de_inputs, test_de_labels, batch_size\n",
    "    )\n",
    "    \n",
    "    print(\"\\n(test) loss: {} - accuracy: {} - bleu: {}\".format(test_loss, test_accuracy, test_bleu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'sos', 'eos', 'ich', 'tom', 'nicht', 'ist', 'das', 'du', 'sie', 'zu', 'es', 'die', 'er', 'der', 'hat', 'in', 'dass', 'ein', 'wir', 'habe', 'was', 'mir', 'sich', 'mit', 'auf', 'wie', 'war', 'mich', 'den', 'und', 'maria', 'ihr', 'eine', 'haben', 'kann', 'an', 'bin', 'sind', 'einen', 'noch', 'so', 'sehr', 'für', 'von', 'dich', 'dir', 'sein', 'als', 'dem', 'hast', 'hier', 'aus', 'um', 'uns', 'nach', 'im', 'wird', 'etwas', 'wenn', 'weiß', 'warum', 'schon', 'meine', 'will', 'bist', 'tun', 'immer', 'gut', 'mehr', 'gehen', 'mein', 'vor', 'bitte', 'werden', 'viel', 'hatte', 'keine', 'wo', 'muss', 'wurde', 'nichts', 'ihn', 'werde', 'machen', 'seine', 'bei', 'heute', 'man', 'diese', 'nie', 'nur', 'am', 'zum', 'möchte', 'jetzt', 'euch', 'dieses', 'können']\n",
      "Training batch 18/5000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [151]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()    \n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mde_vectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds to complete the training\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(t2\u001b[38;5;241m-\u001b[39mt1))\n",
      "Input \u001b[1;32mIn [150]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size)\u001b[0m\n\u001b[0;32m    117\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x, y, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Get the final prediction to compute BLEU\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Update the epoch's log records of the metrics\u001b[39;00m\n\u001b[0;32m    122\u001b[0m loss_log\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\keras\\engine\\training.py:2033\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   2032\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2033\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2034\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   2035\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\manning.tf2.9\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()    \n",
    "train_model(final_model, de_vectorizer, train_df, valid_df, test_df, epochs, batch_size)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"\\nIt took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model\n",
    "\n",
    "We save the trained model as well as the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\seq2seq\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\seq2seq\\assets\n"
     ]
    }
   ],
   "source": [
    "# Section 11.3\n",
    "\n",
    "## Save the model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(final_model, os.path.join('models', 'seq2seq'))\n",
    "\n",
    "import json\n",
    "os.makedirs(os.path.join('models', 'seq2seq_vocab'), exist_ok=True)\n",
    "\n",
    "# Save the vocabulary files\n",
    "with open(os.path.join('models', 'seq2seq_vocab', 'en_vocab.json'), 'w') as f:\n",
    "    json.dump(en_vocabulary, f)    \n",
    "with open(os.path.join('models', 'seq2seq_vocab', 'de_vocab.json'), 'w') as f:\n",
    "    json.dump(de_vocabulary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inference model\n",
    "\n",
    "For inference we have to create a new model using the weights of the trained model. During training we used teacher forcing, i.e. providing words from the translation as inputs to the decoder. This cannot be done during inference as we do not have a translation, but want to generate one.\n",
    "\n",
    "Therefore, we create a decoder model that can generate one prediction at a time. We start the prediction process by giving the `sos` token as the initial input to the decoder and keep generating words until the decoder outputs `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabularies\n",
      "Loading weights and generating the inference model\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "# Section 11.4\n",
    "\n",
    "# Code listing 11.11\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "def get_inference_model(save_path):\n",
    "    \"\"\" Load the saved model and create an inference model from that \"\"\"\n",
    "    \n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model(save_path)\n",
    "    \n",
    "    # Get the encoder model\n",
    "    en_model = model.get_layer(\"encoder\")\n",
    "    \n",
    "    # Define two inputs\n",
    "    # 1. Takes a single word as the input to the decoder\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_infer_input')\n",
    "    # 2. Takes an initial state to pass to the decoder GRU as an input\n",
    "    d_state_inp = tf.keras.Input(shape=(256,), name='d_infer_state')\n",
    "    \n",
    "    # Generate the vectorized output of inp\n",
    "    d_vectorizer = model.get_layer('d_vectorizer')    \n",
    "    d_vectorized_out = d_vectorizer(d_inp)\n",
    "    \n",
    "    # Generate the embeddings from the vectorized input\n",
    "    d_emb_out = model.get_layer('d_embedding')(d_vectorized_out)\n",
    "    \n",
    "    # Get the GRU layer\n",
    "    d_gru_layer = model.get_layer(\"d_gru\")\n",
    "    # Since we generate one word at a time, we will not need the return_sequences\n",
    "    d_gru_layer.return_sequences = False\n",
    "    # Get the GRU out while using d_state_inp from earlier, as the initial state\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_state_inp) \n",
    "    \n",
    "    # Get the dense output\n",
    "    d_dense1_out = model.get_layer(\"d_dense_1\")(d_gru_out) \n",
    "    \n",
    "    # Get the final output\n",
    "    d_final_out = model.get_layer(\"d_dense_final\")(d_dense1_out) \n",
    "    \n",
    "    # Define the final decoder\n",
    "    de_model = tf.keras.models.Model(inputs=[d_inp, d_state_inp], outputs=[d_final_out, d_gru_out])\n",
    "    \n",
    "    return en_model, de_model\n",
    "\n",
    "def get_vocabularies(save_dir):\n",
    "    \"\"\" Load the vocabulary files from a given path\"\"\"\n",
    "    \n",
    "    with open(os.path.join(save_dir, 'en_vocab.json'), 'r') as f:\n",
    "        en_vocabulary = json.load(f)\n",
    "        \n",
    "    with open(os.path.join(save_dir, 'de_vocab.json'), 'r') as f:\n",
    "        de_vocabulary = json.load(f)\n",
    "        \n",
    "    return en_vocabulary, de_vocabulary\n",
    "\n",
    "print(\"Loading vocabularies\")\n",
    "en_vocabulary, de_vocabulary = get_vocabularies(os.path.join('models', 'seq2seq_vocab'))\n",
    "\n",
    "print(\"Loading weights and generating the inference model\")\n",
    "en_model, de_model = get_inference_model(os.path.join('models', 'seq2seq'))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new translations\n",
    "\n",
    "Here we generate a new translation by first starting with the `sos` token and asking the decoder to generate words until it outputs `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Tomorrow's your day off.\n",
      "Translation: er [UNK] [UNK] eos\n",
      "\n",
      "Input: Let's see what's on TV.\n",
      "Translation: du [UNK] eos\n",
      "\n",
      "Input: Don't help a woman in public. You'll look suspicious.\n",
      "Translation: du [UNK] [UNK] eos\n",
      "\n",
      "Input: I'm proud of myself.\n",
      "Translation: ich habe [UNK] eos\n",
      "\n",
      "Input: How did Tom pay the rent?\n",
      "Translation: du [UNK] [UNK] [UNK] eos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code listing 11.12\n",
    "def generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text):\n",
    "    \"\"\" Generate a new translation \"\"\"\n",
    "    \n",
    "    start_token = 'sos'\n",
    "    \n",
    "    # Print the input\n",
    "    print(\"Input: {}\".format(sample_en_text))\n",
    "    \n",
    "    # Get the initial state for the decoder\n",
    "    d_state = en_model.predict(np.array([sample_en_text]), verbose=0)\n",
    "    # First word will be sos\n",
    "    de_word = start_token\n",
    "    # We collect the translation in this list\n",
    "    de_translation = []\n",
    "    \n",
    "    # Keep predicting until we get eos\n",
    "    while de_word != 'eos':\n",
    "        # Override the previous state input with the new state\n",
    "        de_pred, d_state = de_model.predict([np.array([de_word]), d_state], verbose=0)    \n",
    "        # Get the actual word from the token ID of the prediction\n",
    "        de_word = de_vocabulary[np.argmax(de_pred[0])]\n",
    "        # Add that to the translation\n",
    "        de_translation.append(de_word)\n",
    "\n",
    "    print(\"Translation: {}\\n\".format(' '.join(de_translation)))\n",
    "\n",
    "for i in range(5):\n",
    "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
    "    generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
